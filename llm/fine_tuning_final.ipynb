{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f54678-3bdc-4974-ab0d-9e5c7ae2c36f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## í™˜ê²½ ì„¸íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install accelerate --upgrade\n",
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d1a232-d3cc-451c-a857-234a5d9c9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-25.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain)\n",
      "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.19-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.0.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.8.0 (from gradio)\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.3.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.8.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.14)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Downloading openai-1.69.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.1.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.7.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m295.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m210.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "Downloading sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m257.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.11-py3-none-any.whl (60 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
      "Downloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.19-py3-none-any.whl (351 kB)\n",
      "Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m427.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.69.0-py3-none-any.whl (599 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m427.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m193.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m287.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m457.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m235.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, zstandard, websockets, typing-inspection, tomlkit, threadpoolctl, tenacity, shellingham, semantic-version, ruff, python-multipart, python-dotenv, pydantic-core, orjson, numpy, mypy-extensions, mdurl, marshmallow, jsonpatch, joblib, jiter, httpx-sse, h11, groovy, greenlet, ffmpy, click, async-timeout, annotated-types, aiofiles, uvicorn, typing-inspect, tiktoken, starlette, SQLAlchemy, scipy, requests-toolbelt, pydantic, markdown-it-py, httpcore, faiss-cpu, scikit-learn, rich, pydantic-settings, httpx, fastapi, dataclasses-json, typer, safehttpx, openai, langsmith, gradio-client, sentence-transformers, langchain-core, gradio, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "Successfully installed SQLAlchemy-2.0.40 aiofiles-23.2.1 annotated-types-0.7.0 async-timeout-4.0.3 click-8.1.8 dataclasses-json-0.6.7 faiss-cpu-1.10.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.1 gradio-client-1.8.0 greenlet-3.1.1 groovy-0.1.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 jiter-0.9.0 joblib-1.4.2 jsonpatch-1.33 langchain-0.3.21 langchain-community-0.3.20 langchain-core-0.3.49 langchain-openai-0.3.11 langchain-text-splitters-0.3.7 langsmith-0.3.19 markdown-it-py-3.0.0 marshmallow-3.26.1 mdurl-0.1.2 mypy-extensions-1.0.0 numpy-2.2.4 openai-1.69.0 orjson-3.10.16 pydantic-2.11.1 pydantic-core-2.33.0 pydantic-settings-2.8.1 pydub-0.25.1 python-dotenv-1.1.0 python-multipart-0.0.20 requests-toolbelt-1.0.0 rich-13.9.4 ruff-0.11.2 safehttpx-0.1.6 scikit-learn-1.6.1 scipy-1.15.2 semantic-version-2.10.0 sentence-transformers-4.0.1 shellingham-1.5.4 starlette-0.46.1 tenacity-9.0.0 threadpoolctl-3.6.0 tiktoken-0.9.0 tomlkit-0.13.2 typer-0.15.2 typing-inspect-0.9.0 typing-inspection-0.4.0 uvicorn-0.34.0 websockets-15.0.1 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "\n",
    "!pip install \\\n",
    "  torch \\\n",
    "  transformers \\\n",
    "  accelerate \\\n",
    "  peft \\\n",
    "  langchain \\\n",
    "  faiss-cpu \\\n",
    "  gradio \\\n",
    "  sentence-transformers \\\n",
    "  huggingface_hub \\\n",
    "  langchain-community \\\n",
    "  langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cbaa29-15ba-4a5a-8ca9-fa316725b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# âœ… numpy 1.26 ì´ìƒì—ì„œë§Œ sentence-transformersê°€ ì •ìƒ ë™ì‘í•˜ë¯€ë¡œ ë²„ì „ ëª…ì‹œ\n",
    "!pip install --upgrade numpy==1.26.4 scipy scikit-learn --quiet\n",
    "\n",
    "# âœ… ë²¡í„° ê²€ìƒ‰ ë° ì„ë² ë”©ìš©!\n",
    "!pip install sentence-transformers --quiet\n",
    "\n",
    "# âœ… LangChain í•µì‹¬ + community ê¸°ëŠ¥\n",
    "!pip install langchain langchain-community --quiet\n",
    "\n",
    "# âœ… huggingface ëª¨ë¸ ë¡œë”© + gradio UI + FAISSìš©\n",
    "!pip install transformers accelerate gradio faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed76f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c733d",
   "metadata": {},
   "source": [
    "## íŒŒì¸íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbc2de-e5b1-402c-93cf-898a1a7c75e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3837078b05a469eb42d7ae388780aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f02000657b4afaa261d8bd1946ddbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096c966ad436448a93744d93e3b9eace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4794a5c4cc1450b95fde0bd104ce33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ae97c9318e4ef296e76d31221b796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3937026eef4355aa5400198ebfa160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ec6d08fa64c999e564c3c4898448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca42b723eca4f82acee0f559f1a1739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2a057581c94f8c9fc287934bdcc047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186b0d2684a4609bea5c660c4b28b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c32aabcc7248c9b374f95f7bc81675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2972c9c9dfac44ceb125be5920ce8d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d609505acc01429586cbc42105e782f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f001680e3c184d41ab565a38d0b8998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2851/742583032.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.348300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ì§ˆë¬¸:\n",
      "ì¹œê¶Œê³¼ ì–‘ìœ¡ê¶Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "\n",
      "### ë¬¸ì„œ:\n",
      "\n",
      "\n",
      "### ë‹µë³€: ì¹œê¶Œê³¼ ì–‘ìœ¡ê¶Œì€ ê°€ì¡± ê´€ê³„ì—ì„œ ë‘ ê°€ì§€ ë‹¤ë¥¸ ê¶Œí•œì„ ë‚˜íƒ€ë‚´ëŠ” ê°œë…ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì¹œê¶Œì€ ê°€ì¡± ê°„ì˜ ê°ì •ì  ê´€ê³„ì™€ ì˜ì¡´ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ, ê°€ì¡± êµ¬ì„±ì›ë“¤ ê°„ì˜ ì •ì‹ ì  ì§€ì§€ì™€ ì‚¬ë‘ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ê°€ì¡± êµ¬ì„±ì›ë“¤ì´ ì„œë¡œ ì¹œë°€í•˜ê²Œ ì§€ë‚´ë©° ì„œë¡œì— ëŒ€í•œ ê°ì •ì  ì—°ê²°ì„ ìœ ì§€í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì–‘ìœ¡ê¶Œì€ ê°€ì¡± êµ¬ì„±ì›ë“¤ì´ ìë…€ì—ê²Œ ì±…ì„ì„ ì§€ê³  ì–‘ìœ¡ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ ê¶Œí•œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ê°€ì¡± êµ¬ì„±ì›ë“¤\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# âœ… Hugging Face ë¡œê·¸ì¸ (í† í° ì…ë ¥)\n",
    "login(\"\")\n",
    "\n",
    "# âœ… CUDA í™˜ê²½ ì„¤ì • (GPU 1ê°œë§Œ ì‚¬ìš©)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë“œ\n",
    "with open(\"./data/cleaned_familylaw_finetune_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# âœ… ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_id = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def format_prompt(example):\n",
    "    prompt = f\"### ì§ˆë¬¸:\\n{example['instruction']}\\n\\n### ë¬¸ì„œ:\\n{example['input']}\\n\\n### ë‹µë³€:\"\n",
    "    response = example[\"output\"]\n",
    "    full_text = prompt + \" \" + response\n",
    "\n",
    "    # ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ ì²˜ë¦¬\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # ë ˆì´ë¸”ì€ input_ids ë³µì‚¬í•´ì„œ ë‹µë³€ ì „ê¹Œì§€ëŠ” ë§ˆìŠ¤í‚¹\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # prompt ê¸¸ì´ë§Œí¼ -100 ë§ˆìŠ¤í‚¹\n",
    "    prompt_len = len(tokenizer(prompt, truncation=True, max_length=1024)[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt)\n",
    "\n",
    "# âœ… QLoRA ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë“œ (GPU 0ë§Œ ì‚¬ìš©)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# âœ… LoRA ì„¤ì •\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# âœ… í•™ìŠµ ì¸ì\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_openchat_familylaw\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# âœ… Trainer ì •ì˜ ë° í•™ìŠµ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# âœ… ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./qlora_openchat_familylaw_v2/peft_model\")\n",
    "tokenizer.save_pretrained(\"./qlora_openchat_familylaw_v2/peft_model\")\n",
    "\n",
    "# âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸ìš© ì¶”ë¡  í•¨ìˆ˜\n",
    "def infer(instruction, context=\"\"):\n",
    "    model.eval()\n",
    "    prompt = f\"### ì§ˆë¬¸:\\n{instruction}\\n\\n### ë¬¸ì„œ:\\n{context}\\n\\n### ë‹µë³€:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.85,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# âœ… ì˜ˆì‹œ ì¶”ë¡ \n",
    "print(infer(\"ì¹œê¶Œê³¼ ì–‘ìœ¡ê¶Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4f38d",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fba2375-f4b8-40bc-b2a4-c39a15fde4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166d75b345b14925a363984f9cc7723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/skyss/skn-3rd/commit/c70caecd2325764ac7b53ca6b91c55af457ea91a', commit_message='ğŸš€ Upload QLoRA PEFT fine-tuned family law model', commit_description='', oid='c70caecd2325764ac7b53ca6b91c55af457ea91a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/skyss/skn-3rd', endpoint='https://huggingface.co', repo_type='model', repo_id='skyss/skn-3rd'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "repo_id = \"skyss/skn-3rd\"\n",
    "\n",
    "# âœ… ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ\n",
    "folder_path = \"./qlora_openchat_familylaw_v2/peft_model\"\n",
    "\n",
    "# âœ… ì—…ë¡œë“œ ì‹¤í–‰\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=folder_path,\n",
    "    path_in_repo=\".\",  # ë£¨íŠ¸ì— ì—…ë¡œë“œ\n",
    "    commit_message=\"ğŸš€ Upload QLoRA PEFT fine-tuned family law model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ab5ee",
   "metadata": {},
   "source": [
    "### ì‚¬ìš©ì í™”ë©´ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b853fa5-cf2c-4f02-970f-646749cc01cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0344ed5144342f1a385fc33a2fc52d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_2851/1957843816.py:468: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* Running on public URL: https://ff16a9fa3413829833.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ff16a9fa3413829833.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# âœ… í™˜ê²½ ì„¤ì •\n",
    "\n",
    "# âœ… ëª¨ë¸ ë° ë²¡í„° DB\n",
    "model_path = \"./qlora_openchat_familylaw_v2/peft_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == \"cuda\" else -1)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.load_local(\"law_case_db\", embeddings=embedding, allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì •ë³´ ì´ˆê¸°ê°’\n",
    "user_info = {\n",
    "    \"user_id\": str(uuid.uuid4()),\n",
    "    \"marital_status\": \"\",\n",
    "    \"marriage_duration\": \"\",\n",
    "    \"divorce_stage\": \"\",\n",
    "    \"children\": \"\",\n",
    "    \"abuse_history\": \"\",\n",
    "    \"property_range\": \"\",\n",
    "    \"private_question\": False,\n",
    "    \"created_at\": datetime.utcnow(),\n",
    "    \"registered\": False\n",
    "}\n",
    "chat_history = []\n",
    "\n",
    "# âœ… í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\"\\n\", \" \").replace(\"  \", \" \").strip()\n",
    "\n",
    "\n",
    "# âœ… UTF-8 ì•ˆì „ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def safe_utf8(text):\n",
    "    return text.encode(\"utf-8\", \"surrogatepass\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "def register_user(marital_status, m_priv, marriage_duration, d_priv, divorce_stage, ds_priv,\n",
    "                  children, c_priv, abuse_history, a_priv, property_range, p_priv, private_question):\n",
    "    user_info.update({\n",
    "        \"marital_status\": \"ë¹„ê³µê°œ\" if m_priv else marital_status,\n",
    "        \"marriage_duration\": \"ë¹„ê³µê°œ\" if d_priv else marriage_duration,\n",
    "        \"divorce_stage\": \"ë¹„ê³µê°œ\" if ds_priv else divorce_stage,\n",
    "        \"children\": \"ë¹„ê³µê°œ\" if c_priv else children,\n",
    "        \"abuse_history\": \"ë¹„ê³µê°œ\" if a_priv else abuse_history,\n",
    "        \"property_range\": \"ë¹„ê³µê°œ\" if p_priv else property_range,\n",
    "        \"private_question\": private_question,\n",
    "        \"registered\": True\n",
    "    })\n",
    "    return \"âœ… ì‚¬ìš©ì ì •ë³´ê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!\"\n",
    "\n",
    "def deduplicate_text(text):\n",
    "    lines = text.splitlines()\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and line not in seen:\n",
    "            cleaned.append(line)\n",
    "            seen.add(line)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def format_documents_with_metadata(docs):\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        meta = doc.metadata\n",
    "        title = meta.get(\"ì‚¬ê±´ë²ˆí˜¸\") or meta.get(\"ì¡°í•­ë²ˆí˜¸\") or meta.get(\"ë²•ë¥ ëª…\") or meta.get(\"íŒë¡€ì¼ë ¨ë²ˆí˜¸\") or f\"ë¬¸ì„œ {i+1}\"\n",
    "        content = doc.page_content.strip()\n",
    "        formatted_docs.append(f\"ğŸ“„ ë¬¸ì„œ {i+1} ({title}):\\n{content}\")\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "def remove_trailing_phrases(text):\n",
    "    phrases = [\"ìµœëŒ€ 1000ì ì´ë‚´ë¡œ ìš”ì•½ë˜ì—ˆìŠµë‹ˆë‹¤.\", \"ìš”ì•½ì„ ë§ˆì¹©ë‹ˆë‹¤.\", \"ì´ìƒìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\"]\n",
    "    for p in phrases:\n",
    "        if p in text:\n",
    "            text = text.replace(p, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "def format_answer_blocks(text):\n",
    "    # 1. ë²•ì  í•´ì„ê³¼ ë‹µë³€ ìƒì„±ì´ ì—†ë‹¤ë©´ ìš”ì•½ ìƒì„±\n",
    "    if \"### 1. ë²•ì  í•´ì„ê³¼ ë‹µë³€\" not in text:\n",
    "        gpt_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ë‹¤ìŒ ë‚´ìš©ì€ ë²•ì  í•´ì„ì— ëŒ€í•œ ìš”ì•½ì…ë‹ˆë‹¤. ë²•ë¥ ì  ê·¼ê±°ì™€ íŒë¡€ë§Œ ë‚¨ê¸°ê³  ì‹¤ë¬´ ì „ëµ ë° ê°œì¸ì  ê³ ë ¤ëŠ” ì œì™¸í•˜ì„¸ìš”.\"},\n",
    "                {\"role\": \"user\", \"content\": text + \"\\n\\në²•ì  ê·¼ê±°ì™€ íŒë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\"}\n",
    "            ]\n",
    "        )\n",
    "        legal_analysis = gpt_response.choices[0].message.content.strip()\n",
    "        text = f\"### 1. âš–ï¸ ë²•ì  í•´ì„ê³¼ ë‹µë³€\\n{legal_analysis}\"\n",
    "\n",
    "    # 2. ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµ ìƒì„±ì´ ì—†ë‹¤ë©´ ìƒì„±\n",
    "    if \"### 2. ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµ\" not in text:\n",
    "        gpt_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ë‹¤ìŒ ë‚´ìš©ì€ ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµì…ë‹ˆë‹¤. ì´ë¯¸ ì œê³µëœ ë²•ì  í•´ì„(1ë²ˆ)ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì‹¤ë¬´ì  ì ‘ê·¼ê³¼ êµ¬ì²´ì  ëŒ€ì‘ ë°©ì•ˆì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\"},\n",
    "                {\"role\": \"user\", \"content\": text + \"\\n\\në²•ì  í•´ì„ì„ ì œì™¸í•˜ê³  ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"}\n",
    "            ]\n",
    "        )\n",
    "        gpt_strategy = gpt_response.choices[0].message.content.strip()\n",
    "        text += f\"\\n\\n### 2. ğŸ›  ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµ\\n{gpt_strategy}\"\n",
    "\n",
    "    # ë§ˆí¬ êµ¬ë¶„ ì‚½ì…\n",
    "    if \"ğŸŸ \" not in text and \"ğŸŸ¢\" not in text and \"ğŸŸ¡\" not in text:\n",
    "        if \"OpenAI GPT\" in text:\n",
    "            text += \"\\n\\nğŸŸ¡ (OpenAI GPT)\"\n",
    "        else:\n",
    "            text += \"\\n\\nğŸŸ¢ (íŒŒì¸íŠœë‹ ëª¨ë¸)\"\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    processed = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == \"\":\n",
    "            processed.append(\"\")\n",
    "        else:\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip() != \"\":\n",
    "                processed.append(line + \"\\n\")\n",
    "            else:\n",
    "                processed.append(line)\n",
    "    text = \"\".join(processed)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def contains_excessive_repetition(text):\n",
    "    patterns = [\n",
    "        r\"(ë”°ë¼ì„œ .*? ì‚¬ê±´ì˜ íŠ¹ì„±ì— ë”°ë¼.*?[\\.\\n]){2,}\",\n",
    "        r\"(ì´ ë‹µë³€ì€ .*? ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ë²•ì  ì¡°ì–¸ì´ í•„ìš”í•©ë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ì´ì— ë”°ë¼ .*? ê²°ì •ë©ë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ì´ëŸ¬í•œ ê²½ìš° .*? ë²•ë¥  ì „ë¬¸ê°€ì™€ ìƒì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ê²°ë¡ ì ìœ¼ë¡œ .*? ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ì´ëŸ¬í•œ ì‚¬ê±´ì— ëŒ€í•´ì„œëŠ” .*? í•„ìš”í•©ë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ì´ë¥¼ ìœ„í•´ ë²•ì  ì¡°ì–¸ì„ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"((?:ì´ ë‹µë³€ì€|ë²•ë¥  ì „ë¬¸ê°€|ë²•ì  ì¡°ì–¸).*?){3,}\",\n",
    "        r\"(ì´ë¥¼ í†µí•´.*?ì‹ ê³ ê°€ ì •ìƒì ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ìœ„ìë£Œ ì¤€ë¹„ ê³¼ì •ì— ëŒ€í•œ ìƒë‹´ì„ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤[\\.\\n]){2,}\",\n",
    "        r\"(ë²•ì  ê³ ë¬¸ì´ë‚˜ ë²•ì›ì—ì„œ.*?ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤[\\.\\n]){2,}\"\n",
    "    ]\n",
    "    return any(re.search(pattern, text, flags=re.DOTALL) for pattern in patterns)\n",
    "\n",
    "def clean_and_finalize(text):\n",
    "    text = clean_text(text)\n",
    "    text = deduplicate_text(text)\n",
    "    text = remove_trailing_phrases(text)\n",
    "    text = format_answer_blocks(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_case_summary_question(question: str) -> str | None:\n",
    "    match = re.search(r\"ì‚¬ê±´ë²ˆí˜¸[:\\s]*(\\d+[ê°€-í£]+\\d+)\", question)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def build_case_summary_prompt(case_id: str, context: str) -> str:\n",
    "    return f\"\"\"ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ìš”ì•½ AIì…ë‹ˆë‹¤. ì•„ë˜ ì‚¬ê±´ë²ˆí˜¸ {case_id}ì— ëŒ€í•œ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.\n",
    "\n",
    "1ï¸âƒ£ ì‚¬ê±´ ê°œìš”\n",
    "2ï¸âƒ£ í•µì‹¬ ìŸì \n",
    "3ï¸âƒ£ íŒê²° ìš”ì§€\n",
    "4ï¸âƒ£ ê¸°íƒ€ ì°¸ê³  ì‚¬í•­ (ìˆë‹¤ë©´)\n",
    "\n",
    "ğŸ“„ ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ğŸ’¬ ìš”ì•½:\"\"\"\n",
    "\n",
    "# âœ… ì¼ë°˜ ìƒë‹´ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_info\", \"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "ì—­í• : ì „ë¬¸ ê°€ì¡±ë²• ë³€í˜¸ì‚¬ (40ë…„ ê²½ë ¥, ì´í˜¼ ë° ê°€ì‚¬ì†Œì†¡ íŠ¹í™”)\n",
    "\n",
    "ğŸ“Œ ë‹¹ì‹ ì€ ë‹¤ìŒ ì›ì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤:\n",
    "- ë¯¼ë²• ì¡°í•­ ë˜ëŠ” íŒë¡€ì— ê¸°ë°˜í•œ **ë²•ì  í•´ì„**ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
    "- ì‚¬ìš©ìì˜ ê°€ì¡± ìƒí™©ì„ ë°˜ì˜í•˜ì—¬ **ì‹¤ì§ˆì ì¸ ì¡°ì–¸**ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "- ë‹µë³€ì€ ì•„ë˜ ì˜ˆì‹œ í˜•ì‹ì„ ì² ì €íˆ ë”°ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘¤ ì˜ë¢°ì¸ ì •ë³´:\n",
    "- í˜¼ì¸ ìƒíƒœ: {user_info[marital_status]}\n",
    "- í˜¼ì¸ ê¸°ê°„: {user_info[marriage_duration]}\n",
    "- ì´í˜¼ ì§„í–‰ ìƒíƒœ: {user_info[divorce_stage]}\n",
    "- ìë…€ ì •ë³´: {user_info[children]}\n",
    "- ê°€ì •í­ë ¥ ê²½í—˜ : {user_info[abuse_history]}\n",
    "- ì¬ì‚° ë²”ìœ„: {user_info[property_range]}\n",
    "- ë¯¼ê° ì •ë³´ í¬í•¨ ì—¬ë¶€: {user_info[private_question]}\n",
    "\n",
    "â“ ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ğŸ“„ ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¬ [ì˜ˆì‹œ í˜•ì‹] ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "1. ë²•ì  í•´ì„ê³¼ ë‹µë³€  \n",
    "ë¯¼ë²• ì œ840ì¡° ì œ1í˜¸ì— ë”°ë¥´ë©´, **ë°°ìš°ìì˜ ë¶€ì •í–‰ìœ„(ì™¸ë„)**ëŠ” ì¬íŒìƒ ì´í˜¼ ì‚¬ìœ ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ë§Œ, ë‹¨ìˆœ ì™¸ë„ ì‚¬ì‹¤ë§Œìœ¼ë¡œ ì´í˜¼ì´ ì¸ì •ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ë‹¤ìŒ ìš”ê±´ì„ í•¨ê»˜ ë”°ì§‘ë‹ˆë‹¤:\n",
    "- ì™¸ë„ì— ëŒ€í•œ **ì…ì¦ ê°€ëŠ¥í•œ ì¦ê±°**ê°€ ì¡´ì¬í•˜ëŠ”ê°€? (ì‚¬ì§„, ë¬¸ì, ìœ„ì¹˜ê¸°ë¡ ë“±)\n",
    "- ì™¸ë„ ì‚¬ì‹¤ì„ ì•Œê³ ë„ **ìš©ì„œí•˜ê±°ë‚˜ ì¬ë™ê±°**í•œ ì •í™©ì´ ìˆëŠ”ê°€?\n",
    "- í˜¼ì¸ì„ ìœ ì§€í•˜ë ¤ëŠ” **ì˜ì‚¬í‘œí˜„ ë˜ëŠ” ì‹œë„**ê°€ ìˆì—ˆëŠ”ê°€?\n",
    "\n",
    "---\n",
    "\n",
    "2. ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµ  \n",
    "- **ì¦ê±° ìˆ˜ì§‘**: ì™¸ë„ ì •í™©ì— ëŒ€í•œ ë¬¸ì, ìœ„ì¹˜ ê¸°ë¡, SNS ë©”ì‹œì§€ ë“±ì„ í™•ë³´  \n",
    "- **ìš©ì„œ ìœ ë¬´ ì •ë¦¬**: ë™ê±° ì—¬ë¶€, ì¹´í†¡ ë‚´ì—­, ëŒ€í™” ê¸°ë¡ ë“±ì„ ì •ë¦¬í•´ ì†Œì†¡ ëŒ€ì‘ ì¤€ë¹„  \n",
    "- **ì „ë¬¸ê°€ ìƒë‹´**: ë³€í˜¸ì‚¬ì™€ í•¨ê»˜ ì´í˜¼ ì „ëµ ë° ìœ„ìë£Œ ì²­êµ¬ ê°€ëŠ¥ì„± ê²€í†   \n",
    "- **ìë…€/ì¬ì‚° ê³ ë ¤**: ìë…€ê°€ ìˆì„ ê²½ìš° ì–‘ìœ¡ ê³„íš, ì¬ì‚° ë¶„í•  ë°©ì•ˆë„ í•¨ê»˜ ì¤€ë¹„\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“‹ ì°¸ê³  íŒë¡€ ìš”ì•½:\n",
    "- **ëŒ€ë²•ì› 2004ë¯€1234**: ë°˜ë³µëœ ì™¸ë„ëŠ” ëª…ë°±í•œ í˜¼ì¸ íŒŒíƒ„ ì‚¬ìœ ë¡œ ì´í˜¼ ì¸ì •  \n",
    "- **ëŒ€ë²•ì› 2012ë¯€4567**: ì™¸ë„ ì‚¬ì‹¤ì„ ì•Œë©´ì„œ ìš©ì„œí•˜ê³  ë™ê±°í•œ ê²½ìš°, ì´í˜¼ ì²­êµ¬ê°€ ê¸°ê°ë  ìˆ˜ ìˆìŒ\n",
    "\n",
    "---\n",
    "\n",
    "ğŸŸ¢ ìœ„ì™€ ê°™ì€ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬, ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ì‹¤ì œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "[ì¶œë ¥ ê°€ì´ë“œë¼ì¸]\n",
    "- ìµœëŒ€í•œ ê°„ê²°í•˜ê³  ëª…í™•í•œ ì–¸ì–´ ì‚¬ìš©\n",
    "- ì „ë¬¸ ìš©ì–´ëŠ” ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì„¤ëª…\n",
    "- ì •ë³´ì˜ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ êµ¬ì¡°í™”\n",
    "- ì‹œê°ì  êµ¬ë¶„ì„ ìœ„í•´ ì´ëª¨ì§€, ë“¤ì—¬ì“°ê¸°, ì¤„ë°”ê¿ˆ ì ê·¹ í™œìš©\n",
    "- í•µì‹¬ ì •ë³´ëŠ” ë³¼ë“œì²´ë‚˜ íŠ¹ë³„ ê°•ì¡°ë¡œ í‘œì‹œ\n",
    "- ê¸´ í…ìŠ¤íŠ¸ëŠ” ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ì„¸ìš”\n",
    "- ë°˜ë³µ ìµœì†Œí™”\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# âœ… ì§ˆë¬¸ ë¶„ë¥˜\n",
    "\n",
    "def is_general_question_via_llm(question):\n",
    "    try:\n",
    "        res = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ë‹¤ìŒ ì§ˆë¬¸ì´ ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹(ë²• ì¡°í•­ ì„¤ëª…, ê°œë… ì •ì˜, ì ˆì°¨ ì•ˆë‚´ ë“±)ì— ëŒ€í•œ ê²ƒì¸ì§€ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µí•˜ì„¸ìš”.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "        return \"ì˜ˆ\" in res.choices[0].message.content.strip().lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def build_prompt(user_info, question, context):\n",
    "    return prompt_template.format(\n",
    "        user_info=user_info,\n",
    "        question=question,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "def safe_utf8(text):\n",
    "    return text.encode(\"utf-8\", \"surrogatepass\").decode(\"utf-8\", \"ignore\")\n",
    "    \n",
    "\n",
    "# âœ… Cosine ìœ ì‚¬ë„ ê³„ì‚°\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# âœ… íŒŒì¸íŠœë‹ ì´ˆì•ˆ vs GPT ì •ì œ ìœ ì‚¬ë„ ë¹„êµ\n",
    "def is_similar_answer(draft: str, refined: str, threshold: float = 0.945) -> bool:\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vecs = embedding.embed_documents([draft, refined])\n",
    "    return cosine_similarity(vecs[0], vecs[1]) > threshold\n",
    "\n",
    "def filter_docs_with_gpt(question: str, docs: list) -> list:\n",
    "    filtered = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content[:1000]  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°\n",
    "        meta = doc.metadata.get(\"ì‚¬ê±´ë²ˆí˜¸\", \"ë¬¸ì„œ\")  # ë¬¸ì„œ ì œëª©ìš©\n",
    "\n",
    "        check = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"ë‹¹ì‹ ì€ ê°€ì¡±ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œê°€ ì•„ë˜ ì§ˆë¬¸ê³¼ ë²•ë¥ ì  ê´€ì ì—ì„œ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”. \"\n",
    "                        \"'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"ì§ˆë¬¸: {question}\\n\\në¬¸ì„œ ìš”ì•½:\\n{content}\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        result = check.choices[0].message.content.strip().lower()\n",
    "        if \"ì˜ˆ\" in result:\n",
    "            filtered.append(doc)\n",
    "    return filtered\n",
    "\n",
    "def is_law_or_case_summary_question(message: str) -> bool:\n",
    "    return bool(re.search(r\"(ë¯¼ë²•\\\\s?ì œ?\\\\d+ì¡°|í˜•ë²•\\\\s?ì œ?\\\\d+ì¡°|ì¡°ë¬¸|íŒë¡€|ì‚¬ê±´ë²ˆí˜¸)\", message))\n",
    "\n",
    "\n",
    "def filter_relevant_docs(docs, question, threshold=0.7):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    q_vec = embedding.embed_query(question)\n",
    "    kept = []\n",
    "    for doc in docs:\n",
    "        text = (\n",
    "            (doc.metadata.get(\"ë²•ë¥ ëª…\") or \"\") + \" \" +\n",
    "            (doc.metadata.get(\"ì¡°í•­ë²ˆí˜¸\") or \"\") + \" \" +\n",
    "            (doc.metadata.get(\"ì‚¬ê±´ë²ˆí˜¸\") or \"\") + \" \" +\n",
    "            doc.page_content\n",
    "        )\n",
    "        d_vec = embedding.embed_query(text)\n",
    "        sim = cosine_similarity(q_vec, d_vec)\n",
    "        if sim > threshold:\n",
    "            kept.append(doc)\n",
    "    return kept\n",
    "\n",
    "def extract_single_answer(output_text):\n",
    "    # \"### ë‹µë³€:\" ì´í›„ë§Œ ì¶”ì¶œ (ìˆë‹¤ë©´)\n",
    "    if \"### ë‹µë³€:\" in output_text:\n",
    "        return output_text.split(\"### ë‹µë³€:\")[-1].strip()\n",
    "    return output_text.strip()\n",
    "\n",
    "# âœ… ë©”ì¸ ì±—ë´‡ í•¨ìˆ˜\n",
    "def chat_fn(message):\n",
    "    # âœ… 1. ë²• ì¡°ë¬¸ ë˜ëŠ” íŒë¡€ ìš”ì•½ ì§ˆë¬¸ ì²˜ë¦¬\n",
    "    if is_law_or_case_summary_question(message):\n",
    "        gpt_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": (\n",
    "                    \"ë‹¹ì‹ ì€ ë²•ë¥  ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ 4ë‹¨ê³„ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\\n\\n\"\n",
    "                    \"1ï¸âƒ£ ê´€ë ¨ ë²•ë¥  ì¡°í•­\\n2ï¸âƒ£ ì£¼ìš” íŒë¡€ ìš”ì•½\\n3ï¸âƒ£ ì¼ë°˜ì ì¸ ë²•ì  í•´ì„\\n4ï¸âƒ£ ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµ\\n\\n\"\n",
    "                    \"- ë¯¼ë²• ë˜ëŠ” ê°€ì¡±ë²• ì¡°í•­ì„ ëª…í™•í•˜ê²Œ ì œì‹œ\\n\"\n",
    "                    \"- ê´€ë ¨ëœ ëŒ€ë²•ì› íŒë¡€ë‚˜ ì£¼ìš” ì‚¬ë¡€ë¥¼ ìš”ì•½\\n\"\n",
    "                    \"- ìƒí™©ì— ë”°ë¥¸ ë²•ì  í•´ì„ì„ ì„¤ëª…\\n\"\n",
    "                    \"- ì‚¬ìš©ìê°€ ê³ ë ¤í•´ì•¼ í•  ëŒ€ì‘ ì „ëµì„ ëª…í™•í•˜ê²Œ ì •ë¦¬\\n\"\n",
    "                    \"- ì¤‘ë³µ, ë°˜ë³µ ë¬¸êµ¬ëŠ” í”¼í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„œìˆ \"\n",
    "                )},\n",
    "                {\"role\": \"user\", \"content\": message}\n",
    "            ]\n",
    "        )\n",
    "        answer = gpt_response.choices[0].message.content.strip()\n",
    "        chat_history.append((message, f\"{answer}\\n\\nğŸŸ  (íŒŒì¸íŠœë‹ ëª¨ë¸ + GPT)\"))\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    # âœ… 2. ì‚¬ê±´ë²ˆí˜¸ ê¸°ë°˜ ìš”ì•½ ì²˜ë¦¬\n",
    "    case_id = is_case_summary_question(message)\n",
    "    docs = retriever.get_relevant_documents(message)\n",
    "    docs = filter_relevant_docs(docs, message)\n",
    "\n",
    "    if case_id:\n",
    "        filtered_docs = [doc for doc in docs if doc.metadata.get(\"ì‚¬ê±´ë²ˆí˜¸\") == case_id]\n",
    "        if filtered_docs:\n",
    "            docs = filtered_docs\n",
    "        context = format_documents_with_metadata(docs)\n",
    "        prompt = build_case_summary_prompt(case_id, context)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        answer = extract_single_answer(response.choices[0].message.content)\n",
    "        chat_history.append((message, f\"{answer}\\n\\nğŸŸ  (íŒŒì¸íŠœë‹ ëª¨ë¸ + GPT)\\n\\nğŸ“‹ ì°¸ê³  ë¬¸ì„œ:\\n{context}\"))\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    # âœ… 3. ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ + ì„¸ì…˜ ê¸°ë°˜ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰\n",
    "    related_context = \"\"\n",
    "    if chat_history:\n",
    "        q_vec = embedding.embed_query(message)\n",
    "        for past_q, past_a in reversed(chat_history[-5:]):\n",
    "            past_vec = embedding.embed_query(past_q)\n",
    "            sim = cosine_similarity(q_vec, past_vec)\n",
    "            if sim > 0.85:\n",
    "                related_context = \"\"\n",
    "                break\n",
    "\n",
    "    context = related_context + \"\\n\\n\" + format_documents_with_metadata(docs) if docs else related_context\n",
    "    prompt = build_prompt(user_info, message, context)\n",
    "\n",
    "    output = pipe(prompt, max_new_tokens=1500, temperature=0.7, do_sample=True)[0][\"generated_text\"]\n",
    "    draft = extract_single_answer(output[len(prompt):]).strip()\n",
    "\n",
    "    gpt_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ê°€ì¡±ë²• ì „ë¬¸ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ ì´ˆì•ˆì…ë‹ˆë‹¤.\\n\"\n",
    "                \"ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒ 4ë‹¨ê³„ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\\n\\n\"\n",
    "                \"1ï¸âƒ£ ê´€ë ¨ ë²•ë¥  ì¡°í•­\\n2ï¸âƒ£ ì£¼ìš” íŒë¡€ ìš”ì•½\\n3ï¸âƒ£ ì¼ë°˜ì ì¸ ë²•ì  í•´ì„\\n4ï¸âƒ£ ë§ì¶¤í˜• ëŒ€ì‘ ì „ëµ\\n\\n\"\n",
    "                \"- ë¯¼ë²• ë˜ëŠ” ê°€ì¡±ë²• ì¡°í•­ì„ ëª…í™•í•˜ê²Œ ì œì‹œ\\n\"\n",
    "                \"- ê´€ë ¨ëœ ëŒ€ë²•ì› íŒë¡€ë‚˜ ì£¼ìš” ì‚¬ë¡€ë¥¼ ìš”ì•½\\n\"\n",
    "                \"- ìƒí™©ì— ë”°ë¥¸ ë²•ì  í•´ì„ì„ ì„¤ëª…\\n\"\n",
    "                \"- ì‚¬ìš©ìê°€ ê³ ë ¤í•´ì•¼ í•  ëŒ€ì‘ ì „ëµì„ ëª…í™•í•˜ê²Œ ì •ë¦¬\\n\"\n",
    "                \"- ì¤‘ë³µ, ë°˜ë³µ ë¬¸êµ¬ëŠ” í”¼í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„œìˆ \"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": f\"ì§ˆë¬¸: {message}\\n\\níŒŒì¸íŠœë‹ ëª¨ë¸ ì´ˆì•ˆ ì‘ë‹µ:\\n{draft}\"}\n",
    "        ]\n",
    "    )\n",
    "    refined = gpt_response.choices[0].message.content.strip()\n",
    "    mark = \"ğŸŸ¢ (íŒŒì¸íŠœë‹ ëª¨ë¸)\" if is_similar_answer(draft, refined) else \"ğŸŸ  (íŒŒì¸íŠœë‹ ëª¨ë¸ + GPT)\"\n",
    "    final_answer = f\"{refined}\\n\\n{mark}\"\n",
    "    doc_section = f\"\\n\\nğŸ“‹ ì°¸ê³  ë¬¸ì„œ:\\n{context}\" if context else \"\"\n",
    "\n",
    "    chat_history.append((message, final_answer + doc_section))\n",
    "    return chat_history, \"\"\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Gradio UI\n",
    "def build_ui():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## ğŸ“š LawQuick\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"**ğŸ‘¤ ê¸°ë³¸ ì •ë³´**\")\n",
    "                marital_status = gr.Dropdown(label=\"1. í˜¼ì¸ ìƒíƒœ\", choices=[\"ê¸°í˜¼\", \"ì´í˜¼\", \"ë³„ê±°\", \"ì‚¬ì‹¤í˜¼\"])\n",
    "                m_priv = gr.Checkbox(label=\"ë¹„ê³µê°œ ì²˜ë¦¬\")\n",
    "                marriage_duration = gr.Textbox(label=\"2. í˜¼ì¸ ê¸°ê°„ (ì˜ˆ: 10ë…„)\")\n",
    "                d_priv = gr.Checkbox(label=\"ë¹„ê³µê°œ ì²˜ë¦¬\")\n",
    "                divorce_stage = gr.Dropdown(label=\"3. ì´í˜¼ ì§„í–‰ ìƒíƒœ\", choices=[\"ì´í˜¼ ê³ ë ¤ ì¤‘\", \"ì´í˜¼ ì¤€ë¹„ ì¤‘\", \"ì´í˜¼ ì§„í–‰ ì¤‘\", \"ì´ë¯¸ ì´í˜¼í•¨\"])\n",
    "                ds_priv = gr.Checkbox(label=\"ë¹„ê³µê°œ ì²˜ë¦¬\")\n",
    "\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"**ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ê°€ì¡± ë° ì¬ì‚° ì •ë³´**\")\n",
    "                children = gr.Textbox(label=\"4. ìë…€ ì •ë³´ (ì˜ˆ: 2ëª…, 5ì„¸/8ì„¸)\")\n",
    "                c_priv = gr.Checkbox(label=\"ë¹„ê³µê°œ ì²˜ë¦¬\")\n",
    "                abuse_history = gr.Radio(label=\"5. ê°€ì •í­ë ¥ ë˜ëŠ” ì •ì‹ ì  ê³ í†µ ê²½í—˜\", choices=[\"ì—†ìŒ\", \"ìˆìŒ\"], value=\"ì—†ìŒ\")\n",
    "                a_priv = gr.Checkbox(label=\"ë¹„ê³µê°œ ì²˜ë¦¬\")\n",
    "                property_range = gr.Dropdown(label=\"6. ì¬ì‚° ë²”ìœ„\", choices=[\"1ì²œë§Œ ì› ë¯¸ë§Œ\", \"1ì²œë§Œ~5ì²œë§Œ ì›\", \"5ì²œë§Œ~1ì–µ ì›\", \"1ì–µ ì› ì´ìƒ\"])\n",
    "                p_priv = gr.Checkbox(label=\"ë¹„ê³µê°œ ì²˜ë¦¬\")\n",
    "                private_question = gr.Checkbox(label=\"ğŸ›‘ ì§ˆë¬¸ì— ë¯¼ê°í•œ ê°œì¸ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŒ\", value=False)\n",
    "\n",
    "        confirm_btn = gr.Button(\"âœ… ì‚¬ìš©ì ì •ë³´ ë“±ë¡\")\n",
    "        user_info_status = gr.Markdown()\n",
    "        chatbot = gr.Chatbot()\n",
    "        msg = gr.Textbox(label=\"ğŸ’¬ ì§ˆë¬¸ ì…ë ¥\", placeholder=\"ì˜ˆ: ë‚¨í¸ì´ í­í–‰í•˜ë©´ ìœ„ìë£Œ ì–¼ë§ˆë‚˜ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\", lines=2)\n",
    "        ask_btn = gr.Button(\"ì§ˆë¬¸í•˜ê¸°\")\n",
    "        clear_btn = gr.Button(\"ì´ˆê¸°í™”\")\n",
    "\n",
    "        confirm_btn.click(register_user,\n",
    "                          inputs=[marital_status, m_priv, marriage_duration, d_priv, divorce_stage, ds_priv,\n",
    "                                  children, c_priv, abuse_history, a_priv, property_range, p_priv, private_question],\n",
    "                          outputs=[user_info_status])\n",
    "        ask_btn.click(chat_fn, inputs=[msg], outputs=[chatbot, msg])\n",
    "        clear_btn.click(lambda: ([], \"\"), None, [chatbot, msg])\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = build_ui()\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72570b-3f86-4cf5-b9f9-9b4e512989e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
