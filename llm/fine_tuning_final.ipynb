{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f54678-3bdc-4974-ab0d-9e5c7ae2c36f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 환경 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install accelerate --upgrade\n",
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d1a232-d3cc-451c-a857-234a5d9c9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-25.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain)\n",
      "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.19-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.0.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.8.0 (from gradio)\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.3.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.8.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.14)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Downloading openai-1.69.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.1.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.7.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m295.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m210.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "Downloading sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m257.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.11-py3-none-any.whl (60 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
      "Downloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.19-py3-none-any.whl (351 kB)\n",
      "Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m427.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.69.0-py3-none-any.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m427.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m193.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m287.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m457.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m235.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, zstandard, websockets, typing-inspection, tomlkit, threadpoolctl, tenacity, shellingham, semantic-version, ruff, python-multipart, python-dotenv, pydantic-core, orjson, numpy, mypy-extensions, mdurl, marshmallow, jsonpatch, joblib, jiter, httpx-sse, h11, groovy, greenlet, ffmpy, click, async-timeout, annotated-types, aiofiles, uvicorn, typing-inspect, tiktoken, starlette, SQLAlchemy, scipy, requests-toolbelt, pydantic, markdown-it-py, httpcore, faiss-cpu, scikit-learn, rich, pydantic-settings, httpx, fastapi, dataclasses-json, typer, safehttpx, openai, langsmith, gradio-client, sentence-transformers, langchain-core, gradio, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "Successfully installed SQLAlchemy-2.0.40 aiofiles-23.2.1 annotated-types-0.7.0 async-timeout-4.0.3 click-8.1.8 dataclasses-json-0.6.7 faiss-cpu-1.10.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.1 gradio-client-1.8.0 greenlet-3.1.1 groovy-0.1.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 jiter-0.9.0 joblib-1.4.2 jsonpatch-1.33 langchain-0.3.21 langchain-community-0.3.20 langchain-core-0.3.49 langchain-openai-0.3.11 langchain-text-splitters-0.3.7 langsmith-0.3.19 markdown-it-py-3.0.0 marshmallow-3.26.1 mdurl-0.1.2 mypy-extensions-1.0.0 numpy-2.2.4 openai-1.69.0 orjson-3.10.16 pydantic-2.11.1 pydantic-core-2.33.0 pydantic-settings-2.8.1 pydub-0.25.1 python-dotenv-1.1.0 python-multipart-0.0.20 requests-toolbelt-1.0.0 rich-13.9.4 ruff-0.11.2 safehttpx-0.1.6 scikit-learn-1.6.1 scipy-1.15.2 semantic-version-2.10.0 sentence-transformers-4.0.1 shellingham-1.5.4 starlette-0.46.1 tenacity-9.0.0 threadpoolctl-3.6.0 tiktoken-0.9.0 tomlkit-0.13.2 typer-0.15.2 typing-inspect-0.9.0 typing-inspection-0.4.0 uvicorn-0.34.0 websockets-15.0.1 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "\n",
    "!pip install \\\n",
    "  torch \\\n",
    "  transformers \\\n",
    "  accelerate \\\n",
    "  peft \\\n",
    "  langchain \\\n",
    "  faiss-cpu \\\n",
    "  gradio \\\n",
    "  sentence-transformers \\\n",
    "  huggingface_hub \\\n",
    "  langchain-community \\\n",
    "  langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cbaa29-15ba-4a5a-8ca9-fa316725b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ✅ numpy 1.26 이상에서만 sentence-transformers가 정상 동작하므로 버전 명시\n",
    "!pip install --upgrade numpy==1.26.4 scipy scikit-learn --quiet\n",
    "\n",
    "# ✅ 벡터 검색 및 임베딩용!\n",
    "!pip install sentence-transformers --quiet\n",
    "\n",
    "# ✅ LangChain 핵심 + community 기능\n",
    "!pip install langchain langchain-community --quiet\n",
    "\n",
    "# ✅ huggingface 모델 로딩 + gradio UI + FAISS용\n",
    "!pip install transformers accelerate gradio faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed76f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c733d",
   "metadata": {},
   "source": [
    "## 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbc2de-e5b1-402c-93cf-898a1a7c75e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3837078b05a469eb42d7ae388780aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f02000657b4afaa261d8bd1946ddbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096c966ad436448a93744d93e3b9eace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4794a5c4cc1450b95fde0bd104ce33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ae97c9318e4ef296e76d31221b796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3937026eef4355aa5400198ebfa160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ec6d08fa64c999e564c3c4898448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca42b723eca4f82acee0f559f1a1739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2a057581c94f8c9fc287934bdcc047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186b0d2684a4609bea5c660c4b28b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c32aabcc7248c9b374f95f7bc81675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2972c9c9dfac44ceb125be5920ce8d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d609505acc01429586cbc42105e782f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f001680e3c184d41ab565a38d0b8998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2851/742583032.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.348300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문:\n",
      "친권과 양육권의 차이점은 무엇인가요?\n",
      "\n",
      "### 문서:\n",
      "\n",
      "\n",
      "### 답변: 친권과 양육권은 가족 관계에서 두 가지 다른 권한을 나타내는 개념입니다.\n",
      "\n",
      "친권은 가족 간의 감정적 관계와 의존성을 나타내는 것으로, 가족 구성원들 간의 정신적 지지와 사랑을 의미합니다. 이는 가족 구성원들이 서로 친밀하게 지내며 서로에 대한 감정적 연결을 유지하는 것을 의미합니다.\n",
      "\n",
      "양육권은 가족 구성원들이 자녀에게 책임을 지고 양육 역할을 수행하는 데 필요한 권한을 의미합니다. 이는 가족 구성원들\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ✅ Hugging Face 로그인 (토큰 입력)\n",
    "login(\"\")\n",
    "\n",
    "# ✅ CUDA 환경 설정 (GPU 1개만 사용)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ✅ 데이터 로드\n",
    "with open(\"./data/cleaned_familylaw_finetune_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# ✅ 모델/토크나이저 로드\n",
    "model_id = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ✅ 전처리 함수 정의\n",
    "def format_prompt(example):\n",
    "    prompt = f\"### 질문:\\n{example['instruction']}\\n\\n### 문서:\\n{example['input']}\\n\\n### 답변:\"\n",
    "    response = example[\"output\"]\n",
    "    full_text = prompt + \" \" + response\n",
    "\n",
    "    # 전체 시퀀스를 하나로 처리\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # 레이블은 input_ids 복사해서 답변 전까지는 마스킹\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # prompt 길이만큼 -100 마스킹\n",
    "    prompt_len = len(tokenizer(prompt, truncation=True, max_length=1024)[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt)\n",
    "\n",
    "# ✅ QLoRA 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# ✅ 모델 로드 (GPU 0만 사용)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# ✅ LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# ✅ 학습 인자\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_openchat_familylaw\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ✅ Trainer 정의 및 학습\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ✅ 모델 저장\n",
    "model.save_pretrained(\"./qlora_openchat_familylaw_v2/peft_model\")\n",
    "tokenizer.save_pretrained(\"./qlora_openchat_familylaw_v2/peft_model\")\n",
    "\n",
    "# ✅ 파인튜닝 모델 테스트용 추론 함수\n",
    "def infer(instruction, context=\"\"):\n",
    "    model.eval()\n",
    "    prompt = f\"### 질문:\\n{instruction}\\n\\n### 문서:\\n{context}\\n\\n### 답변:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.85,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ✅ 예시 추론\n",
    "print(infer(\"친권과 양육권의 차이점은 무엇인가요?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4f38d",
   "metadata": {},
   "source": [
    "## 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fba2375-f4b8-40bc-b2a4-c39a15fde4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166d75b345b14925a363984f9cc7723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/skyss/skn-3rd/commit/c70caecd2325764ac7b53ca6b91c55af457ea91a', commit_message='🚀 Upload QLoRA PEFT fine-tuned family law model', commit_description='', oid='c70caecd2325764ac7b53ca6b91c55af457ea91a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/skyss/skn-3rd', endpoint='https://huggingface.co', repo_type='model', repo_id='skyss/skn-3rd'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "repo_id = \"skyss/skn-3rd\"\n",
    "\n",
    "# ✅ 저장된 모델 경로\n",
    "folder_path = \"./qlora_openchat_familylaw_v2/peft_model\"\n",
    "\n",
    "# ✅ 업로드 실행\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=folder_path,\n",
    "    path_in_repo=\".\",  # 루트에 업로드\n",
    "    commit_message=\"🚀 Upload QLoRA PEFT fine-tuned family law model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ab5ee",
   "metadata": {},
   "source": [
    "### 사용자 화면 및 프롬프트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b853fa5-cf2c-4f02-970f-646749cc01cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0344ed5144342f1a385fc33a2fc52d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_2851/1957843816.py:468: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* Running on public URL: https://ff16a9fa3413829833.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ff16a9fa3413829833.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 환경 설정\n",
    "\n",
    "# ✅ 모델 및 벡터 DB\n",
    "model_path = \"./qlora_openchat_familylaw_v2/peft_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == \"cuda\" else -1)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.load_local(\"law_case_db\", embeddings=embedding, allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# ✅ 사용자 정보 초기값\n",
    "user_info = {\n",
    "    \"user_id\": str(uuid.uuid4()),\n",
    "    \"marital_status\": \"\",\n",
    "    \"marriage_duration\": \"\",\n",
    "    \"divorce_stage\": \"\",\n",
    "    \"children\": \"\",\n",
    "    \"abuse_history\": \"\",\n",
    "    \"property_range\": \"\",\n",
    "    \"private_question\": False,\n",
    "    \"created_at\": datetime.utcnow(),\n",
    "    \"registered\": False\n",
    "}\n",
    "chat_history = []\n",
    "\n",
    "# ✅ 텍스트 후처리 함수\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\"\\n\", \" \").replace(\"  \", \" \").strip()\n",
    "\n",
    "\n",
    "# ✅ UTF-8 안전 처리 함수\n",
    "def safe_utf8(text):\n",
    "    return text.encode(\"utf-8\", \"surrogatepass\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "def register_user(marital_status, m_priv, marriage_duration, d_priv, divorce_stage, ds_priv,\n",
    "                  children, c_priv, abuse_history, a_priv, property_range, p_priv, private_question):\n",
    "    user_info.update({\n",
    "        \"marital_status\": \"비공개\" if m_priv else marital_status,\n",
    "        \"marriage_duration\": \"비공개\" if d_priv else marriage_duration,\n",
    "        \"divorce_stage\": \"비공개\" if ds_priv else divorce_stage,\n",
    "        \"children\": \"비공개\" if c_priv else children,\n",
    "        \"abuse_history\": \"비공개\" if a_priv else abuse_history,\n",
    "        \"property_range\": \"비공개\" if p_priv else property_range,\n",
    "        \"private_question\": private_question,\n",
    "        \"registered\": True\n",
    "    })\n",
    "    return \"✅ 사용자 정보가 성공적으로 등록되었습니다!\"\n",
    "\n",
    "def deduplicate_text(text):\n",
    "    lines = text.splitlines()\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and line not in seen:\n",
    "            cleaned.append(line)\n",
    "            seen.add(line)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def format_documents_with_metadata(docs):\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        meta = doc.metadata\n",
    "        title = meta.get(\"사건번호\") or meta.get(\"조항번호\") or meta.get(\"법률명\") or meta.get(\"판례일련번호\") or f\"문서 {i+1}\"\n",
    "        content = doc.page_content.strip()\n",
    "        formatted_docs.append(f\"📄 문서 {i+1} ({title}):\\n{content}\")\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "def remove_trailing_phrases(text):\n",
    "    phrases = [\"최대 1000자 이내로 요약되었습니다.\", \"요약을 마칩니다.\", \"이상으로 요약합니다.\"]\n",
    "    for p in phrases:\n",
    "        if p in text:\n",
    "            text = text.replace(p, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "def format_answer_blocks(text):\n",
    "    # 1. 법적 해석과 답변 생성이 없다면 요약 생성\n",
    "    if \"### 1. 법적 해석과 답변\" not in text:\n",
    "        gpt_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"다음 내용은 법적 해석에 대한 요약입니다. 법률적 근거와 판례만 남기고 실무 전략 및 개인적 고려는 제외하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": text + \"\\n\\n법적 근거와 판례를 중심으로 요약해주세요.\"}\n",
    "            ]\n",
    "        )\n",
    "        legal_analysis = gpt_response.choices[0].message.content.strip()\n",
    "        text = f\"### 1. ⚖️ 법적 해석과 답변\\n{legal_analysis}\"\n",
    "\n",
    "    # 2. 맞춤형 대응 전략 생성이 없다면 생성\n",
    "    if \"### 2. 맞춤형 대응 전략\" not in text:\n",
    "        gpt_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"다음 내용은 맞춤형 대응 전략입니다. 이미 제공된 법적 해석(1번)과 중복되지 않도록 실무적 접근과 구체적 대응 방안을 중심으로 작성하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": text + \"\\n\\n법적 해석을 제외하고 맞춤형 대응 전략을 작성해주세요.\"}\n",
    "            ]\n",
    "        )\n",
    "        gpt_strategy = gpt_response.choices[0].message.content.strip()\n",
    "        text += f\"\\n\\n### 2. 🛠 맞춤형 대응 전략\\n{gpt_strategy}\"\n",
    "\n",
    "    # 마크 구분 삽입\n",
    "    if \"🟠\" not in text and \"🟢\" not in text and \"🟡\" not in text:\n",
    "        if \"OpenAI GPT\" in text:\n",
    "            text += \"\\n\\n🟡 (OpenAI GPT)\"\n",
    "        else:\n",
    "            text += \"\\n\\n🟢 (파인튜닝 모델)\"\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    processed = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == \"\":\n",
    "            processed.append(\"\")\n",
    "        else:\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip() != \"\":\n",
    "                processed.append(line + \"\\n\")\n",
    "            else:\n",
    "                processed.append(line)\n",
    "    text = \"\".join(processed)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def contains_excessive_repetition(text):\n",
    "    patterns = [\n",
    "        r\"(따라서 .*? 사건의 특성에 따라.*?[\\.\\n]){2,}\",\n",
    "        r\"(이 답변은 .*? 전문가와 상담하는 것이 좋습니다[\\.\\n]){2,}\",\n",
    "        r\"(법적 조언이 필요합니다[\\.\\n]){2,}\",\n",
    "        r\"(이에 따라 .*? 결정됩니다[\\.\\n]){2,}\",\n",
    "        r\"(이러한 경우 .*? 법률 전문가와 상의하시기 바랍니다[\\.\\n]){2,}\",\n",
    "        r\"(결론적으로 .*? 달라질 수 있습니다[\\.\\n]){2,}\",\n",
    "        r\"(이러한 사건에 대해서는 .*? 필요합니다[\\.\\n]){2,}\",\n",
    "        r\"(이를 위해 법적 조언을 받으시기 바랍니다[\\.\\n]){2,}\",\n",
    "        r\"((?:이 답변은|법률 전문가|법적 조언).*?){3,}\",\n",
    "        r\"(이를 통해.*?신고가 정상적으로 이루어질 수 있습니다[\\.\\n]){2,}\",\n",
    "        r\"(위자료 준비 과정에 대한 상담을 받으시기 바랍니다[\\.\\n]){2,}\",\n",
    "        r\"(법적 고문이나 법원에서.*?받으시기 바랍니다[\\.\\n]){2,}\"\n",
    "    ]\n",
    "    return any(re.search(pattern, text, flags=re.DOTALL) for pattern in patterns)\n",
    "\n",
    "def clean_and_finalize(text):\n",
    "    text = clean_text(text)\n",
    "    text = deduplicate_text(text)\n",
    "    text = remove_trailing_phrases(text)\n",
    "    text = format_answer_blocks(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_case_summary_question(question: str) -> str | None:\n",
    "    match = re.search(r\"사건번호[:\\s]*(\\d+[가-힣]+\\d+)\", question)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def build_case_summary_prompt(case_id: str, context: str) -> str:\n",
    "    return f\"\"\"당신은 법률 문서 요약 AI입니다. 아래 사건번호 {case_id}에 대한 내용을 간결하고 명확하게 요약하세요.\n",
    "\n",
    "1️⃣ 사건 개요\n",
    "2️⃣ 핵심 쟁점\n",
    "3️⃣ 판결 요지\n",
    "4️⃣ 기타 참고 사항 (있다면)\n",
    "\n",
    "📄 문서:\n",
    "{context}\n",
    "\n",
    "💬 요약:\"\"\"\n",
    "\n",
    "# ✅ 일반 상담용 프롬프트 템플릿\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_info\", \"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "역할: 전문 가족법 변호사 (40년 경력, 이혼 및 가사소송 특화)\n",
    "\n",
    "📌 당신은 다음 원칙을 기반으로 응답합니다:\n",
    "- 민법 조항 또는 판례에 기반한 **법적 해석**을 제시합니다.\n",
    "- 사용자의 가족 상황을 반영하여 **실질적인 조언**을 제공합니다.\n",
    "- 답변은 아래 예시 형식을 철저히 따릅니다.\n",
    "\n",
    "---\n",
    "\n",
    "👤 의뢰인 정보:\n",
    "- 혼인 상태: {user_info[marital_status]}\n",
    "- 혼인 기간: {user_info[marriage_duration]}\n",
    "- 이혼 진행 상태: {user_info[divorce_stage]}\n",
    "- 자녀 정보: {user_info[children]}\n",
    "- 가정폭력 경험 : {user_info[abuse_history]}\n",
    "- 재산 범위: {user_info[property_range]}\n",
    "- 민감 정보 포함 여부: {user_info[private_question]}\n",
    "\n",
    "❓ 질문:\n",
    "{question}\n",
    "\n",
    "📄 참고 문서:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "💬 [예시 형식] 다음과 같은 형식으로 답변을 작성하세요:\n",
    "\n",
    "1. 법적 해석과 답변  \n",
    "민법 제840조 제1호에 따르면, **배우자의 부정행위(외도)**는 재판상 이혼 사유에 해당합니다.\n",
    "\n",
    "다만, 단순 외도 사실만으로 이혼이 인정되는 것은 아니며, 다음 요건을 함께 따집니다:\n",
    "- 외도에 대한 **입증 가능한 증거**가 존재하는가? (사진, 문자, 위치기록 등)\n",
    "- 외도 사실을 알고도 **용서하거나 재동거**한 정황이 있는가?\n",
    "- 혼인을 유지하려는 **의사표현 또는 시도**가 있었는가?\n",
    "\n",
    "---\n",
    "\n",
    "2. 맞춤형 대응 전략  \n",
    "- **증거 수집**: 외도 정황에 대한 문자, 위치 기록, SNS 메시지 등을 확보  \n",
    "- **용서 유무 정리**: 동거 여부, 카톡 내역, 대화 기록 등을 정리해 소송 대응 준비  \n",
    "- **전문가 상담**: 변호사와 함께 이혼 전략 및 위자료 청구 가능성 검토  \n",
    "- **자녀/재산 고려**: 자녀가 있을 경우 양육 계획, 재산 분할 방안도 함께 준비\n",
    "\n",
    "---\n",
    "\n",
    "📋 참고 판례 요약:\n",
    "- **대법원 2004므1234**: 반복된 외도는 명백한 혼인 파탄 사유로 이혼 인정  \n",
    "- **대법원 2012므4567**: 외도 사실을 알면서 용서하고 동거한 경우, 이혼 청구가 기각될 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "🟢 위와 같은 형식을 그대로 유지하여, 사용자 질문에 맞는 실제 답변을 작성하세요.\n",
    "\n",
    "[출력 가이드라인]\n",
    "- 최대한 간결하고 명확한 언어 사용\n",
    "- 전문 용어는 일반인도 이해할 수 있게 설명\n",
    "- 정보의 우선순위에 따라 구조화\n",
    "- 시각적 구분을 위해 이모지, 들여쓰기, 줄바꿈 적극 활용\n",
    "- 핵심 정보는 볼드체나 특별 강조로 표시\n",
    "- 긴 텍스트는 간결하고 명확한 문장으로 압축하세요\n",
    "- 반복 최소화\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 질문 분류\n",
    "\n",
    "def is_general_question_via_llm(question):\n",
    "    try:\n",
    "        res = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"다음 질문이 일반적인 법률 지식(법 조항 설명, 개념 정의, 절차 안내 등)에 대한 것인지 '예' 또는 '아니오'로만 답하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "        return \"예\" in res.choices[0].message.content.strip().lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def build_prompt(user_info, question, context):\n",
    "    return prompt_template.format(\n",
    "        user_info=user_info,\n",
    "        question=question,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "def safe_utf8(text):\n",
    "    return text.encode(\"utf-8\", \"surrogatepass\").decode(\"utf-8\", \"ignore\")\n",
    "    \n",
    "\n",
    "# ✅ Cosine 유사도 계산\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# ✅ 파인튜닝 초안 vs GPT 정제 유사도 비교\n",
    "def is_similar_answer(draft: str, refined: str, threshold: float = 0.945) -> bool:\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vecs = embedding.embed_documents([draft, refined])\n",
    "    return cosine_similarity(vecs[0], vecs[1]) > threshold\n",
    "\n",
    "def filter_docs_with_gpt(question: str, docs: list) -> list:\n",
    "    filtered = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content[:1000]  # 너무 길면 자르기\n",
    "        meta = doc.metadata.get(\"사건번호\", \"문서\")  # 문서 제목용\n",
    "\n",
    "        check = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"당신은 가족법 전문가입니다. 다음 문서가 아래 질문과 법률적 관점에서 직접적인 관련이 있는지 판단하세요. \"\n",
    "                        \"'예' 또는 '아니오'만 정확히 출력하세요.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"질문: {question}\\n\\n문서 요약:\\n{content}\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        result = check.choices[0].message.content.strip().lower()\n",
    "        if \"예\" in result:\n",
    "            filtered.append(doc)\n",
    "    return filtered\n",
    "\n",
    "def is_law_or_case_summary_question(message: str) -> bool:\n",
    "    return bool(re.search(r\"(민법\\\\s?제?\\\\d+조|형법\\\\s?제?\\\\d+조|조문|판례|사건번호)\", message))\n",
    "\n",
    "\n",
    "def filter_relevant_docs(docs, question, threshold=0.7):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    q_vec = embedding.embed_query(question)\n",
    "    kept = []\n",
    "    for doc in docs:\n",
    "        text = (\n",
    "            (doc.metadata.get(\"법률명\") or \"\") + \" \" +\n",
    "            (doc.metadata.get(\"조항번호\") or \"\") + \" \" +\n",
    "            (doc.metadata.get(\"사건번호\") or \"\") + \" \" +\n",
    "            doc.page_content\n",
    "        )\n",
    "        d_vec = embedding.embed_query(text)\n",
    "        sim = cosine_similarity(q_vec, d_vec)\n",
    "        if sim > threshold:\n",
    "            kept.append(doc)\n",
    "    return kept\n",
    "\n",
    "def extract_single_answer(output_text):\n",
    "    # \"### 답변:\" 이후만 추출 (있다면)\n",
    "    if \"### 답변:\" in output_text:\n",
    "        return output_text.split(\"### 답변:\")[-1].strip()\n",
    "    return output_text.strip()\n",
    "\n",
    "# ✅ 메인 챗봇 함수\n",
    "def chat_fn(message):\n",
    "    # ✅ 1. 법 조문 또는 판례 요약 질문 처리\n",
    "    if is_law_or_case_summary_question(message):\n",
    "        gpt_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": (\n",
    "                    \"당신은 법률 요약 전문가입니다. 다음과 같은 4단계 형식으로 출력하세요:\\n\\n\"\n",
    "                    \"1️⃣ 관련 법률 조항\\n2️⃣ 주요 판례 요약\\n3️⃣ 일반적인 법적 해석\\n4️⃣ 맞춤형 대응 전략\\n\\n\"\n",
    "                    \"- 민법 또는 가족법 조항을 명확하게 제시\\n\"\n",
    "                    \"- 관련된 대법원 판례나 주요 사례를 요약\\n\"\n",
    "                    \"- 상황에 따른 법적 해석을 설명\\n\"\n",
    "                    \"- 사용자가 고려해야 할 대응 전략을 명확하게 정리\\n\"\n",
    "                    \"- 중복, 반복 문구는 피하고 논리적으로 간결하게 서술\"\n",
    "                )},\n",
    "                {\"role\": \"user\", \"content\": message}\n",
    "            ]\n",
    "        )\n",
    "        answer = gpt_response.choices[0].message.content.strip()\n",
    "        chat_history.append((message, f\"{answer}\\n\\n🟠 (파인튜닝 모델 + GPT)\"))\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    # ✅ 2. 사건번호 기반 요약 처리\n",
    "    case_id = is_case_summary_question(message)\n",
    "    docs = retriever.get_relevant_documents(message)\n",
    "    docs = filter_relevant_docs(docs, message)\n",
    "\n",
    "    if case_id:\n",
    "        filtered_docs = [doc for doc in docs if doc.metadata.get(\"사건번호\") == case_id]\n",
    "        if filtered_docs:\n",
    "            docs = filtered_docs\n",
    "        context = format_documents_with_metadata(docs)\n",
    "        prompt = build_case_summary_prompt(case_id, context)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 법률 문서 요약 전문가입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        answer = extract_single_answer(response.choices[0].message.content)\n",
    "        chat_history.append((message, f\"{answer}\\n\\n🟠 (파인튜닝 모델 + GPT)\\n\\n📋 참고 문서:\\n{context}\"))\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    # ✅ 3. 일반 질문 처리 + 세션 기반 유사 질문 검색\n",
    "    related_context = \"\"\n",
    "    if chat_history:\n",
    "        q_vec = embedding.embed_query(message)\n",
    "        for past_q, past_a in reversed(chat_history[-5:]):\n",
    "            past_vec = embedding.embed_query(past_q)\n",
    "            sim = cosine_similarity(q_vec, past_vec)\n",
    "            if sim > 0.85:\n",
    "                related_context = \"\"\n",
    "                break\n",
    "\n",
    "    context = related_context + \"\\n\\n\" + format_documents_with_metadata(docs) if docs else related_context\n",
    "    prompt = build_prompt(user_info, message, context)\n",
    "\n",
    "    output = pipe(prompt, max_new_tokens=1500, temperature=0.7, do_sample=True)[0][\"generated_text\"]\n",
    "    draft = extract_single_answer(output[len(prompt):]).strip()\n",
    "\n",
    "    gpt_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"당신은 20년 경력의 가족법 전문 변호사입니다. 아래는 파인튜닝 모델이 생성한 응답 초안입니다.\\n\"\n",
    "                \"아래 질문에 대해 다음 4단계 형식으로 출력하세요:\\n\\n\"\n",
    "                \"1️⃣ 관련 법률 조항\\n2️⃣ 주요 판례 요약\\n3️⃣ 일반적인 법적 해석\\n4️⃣ 맞춤형 대응 전략\\n\\n\"\n",
    "                \"- 민법 또는 가족법 조항을 명확하게 제시\\n\"\n",
    "                \"- 관련된 대법원 판례나 주요 사례를 요약\\n\"\n",
    "                \"- 상황에 따른 법적 해석을 설명\\n\"\n",
    "                \"- 사용자가 고려해야 할 대응 전략을 명확하게 정리\\n\"\n",
    "                \"- 중복, 반복 문구는 피하고 논리적으로 간결하게 서술\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": f\"질문: {message}\\n\\n파인튜닝 모델 초안 응답:\\n{draft}\"}\n",
    "        ]\n",
    "    )\n",
    "    refined = gpt_response.choices[0].message.content.strip()\n",
    "    mark = \"🟢 (파인튜닝 모델)\" if is_similar_answer(draft, refined) else \"🟠 (파인튜닝 모델 + GPT)\"\n",
    "    final_answer = f\"{refined}\\n\\n{mark}\"\n",
    "    doc_section = f\"\\n\\n📋 참고 문서:\\n{context}\" if context else \"\"\n",
    "\n",
    "    chat_history.append((message, final_answer + doc_section))\n",
    "    return chat_history, \"\"\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Gradio UI\n",
    "def build_ui():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## 📚 LawQuick\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"**👤 기본 정보**\")\n",
    "                marital_status = gr.Dropdown(label=\"1. 혼인 상태\", choices=[\"기혼\", \"이혼\", \"별거\", \"사실혼\"])\n",
    "                m_priv = gr.Checkbox(label=\"비공개 처리\")\n",
    "                marriage_duration = gr.Textbox(label=\"2. 혼인 기간 (예: 10년)\")\n",
    "                d_priv = gr.Checkbox(label=\"비공개 처리\")\n",
    "                divorce_stage = gr.Dropdown(label=\"3. 이혼 진행 상태\", choices=[\"이혼 고려 중\", \"이혼 준비 중\", \"이혼 진행 중\", \"이미 이혼함\"])\n",
    "                ds_priv = gr.Checkbox(label=\"비공개 처리\")\n",
    "\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"**👨‍👩‍👧‍👦 가족 및 재산 정보**\")\n",
    "                children = gr.Textbox(label=\"4. 자녀 정보 (예: 2명, 5세/8세)\")\n",
    "                c_priv = gr.Checkbox(label=\"비공개 처리\")\n",
    "                abuse_history = gr.Radio(label=\"5. 가정폭력 또는 정신적 고통 경험\", choices=[\"없음\", \"있음\"], value=\"없음\")\n",
    "                a_priv = gr.Checkbox(label=\"비공개 처리\")\n",
    "                property_range = gr.Dropdown(label=\"6. 재산 범위\", choices=[\"1천만 원 미만\", \"1천만~5천만 원\", \"5천만~1억 원\", \"1억 원 이상\"])\n",
    "                p_priv = gr.Checkbox(label=\"비공개 처리\")\n",
    "                private_question = gr.Checkbox(label=\"🛑 질문에 민감한 개인정보가 포함되어 있음\", value=False)\n",
    "\n",
    "        confirm_btn = gr.Button(\"✅ 사용자 정보 등록\")\n",
    "        user_info_status = gr.Markdown()\n",
    "        chatbot = gr.Chatbot()\n",
    "        msg = gr.Textbox(label=\"💬 질문 입력\", placeholder=\"예: 남편이 폭행하면 위자료 얼마나 받을 수 있나요?\", lines=2)\n",
    "        ask_btn = gr.Button(\"질문하기\")\n",
    "        clear_btn = gr.Button(\"초기화\")\n",
    "\n",
    "        confirm_btn.click(register_user,\n",
    "                          inputs=[marital_status, m_priv, marriage_duration, d_priv, divorce_stage, ds_priv,\n",
    "                                  children, c_priv, abuse_history, a_priv, property_range, p_priv, private_question],\n",
    "                          outputs=[user_info_status])\n",
    "        ask_btn.click(chat_fn, inputs=[msg], outputs=[chatbot, msg])\n",
    "        clear_btn.click(lambda: ([], \"\"), None, [chatbot, msg])\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = build_ui()\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72570b-3f86-4cf5-b9f9-9b4e512989e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
