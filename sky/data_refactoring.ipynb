{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ íŒë¡€ ìˆ˜: 9654\n",
      "íŒë¡€ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ” í•­ëª© ìˆ˜: 4105\n",
      "íŒë¡€ë‚´ìš© í•­ëª© ìˆ˜: 5549\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1. JSON íŒŒì¼ ì—´ê¸°\n",
    "with open(\"./data/filtered_case_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. íŒë¡€ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ” í•­ëª©ê³¼ ë¹„ì–´ìˆì§€ ì•Šì€ í•­ëª© ë‚˜ëˆ„ê¸°\n",
    "non_empty_case_contents = [case for case in data if case.get(\"íŒë¡€ë‚´ìš©\")]\n",
    "\n",
    "# 3. ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(\"./data/filtered_case_data_non_empty.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(non_empty_case_contents, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 4. í™•ì¸ ì¶œë ¥\n",
    "print(f\"ì „ì²´ íŒë¡€ ìˆ˜: {len(data)}\")\n",
    "print(f\"íŒë¡€ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ í•­ëª© ìˆ˜: {len(non_empty_case_contents)}\")\n",
    "print(\"âœ… filtered_case_data_non_empty.json íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¡°ë¬¸ í•­ë³„ ë¶„í•  ì™„ë£Œ! cleaned_law_data_preprocessed.json ì— ì €ì¥ë¨.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# í•­ëª© ë¶„ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def split_articles(text):\n",
    "    matches = list(re.finditer(r'(â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©)', text))\n",
    "    if not matches:\n",
    "        return {\"ì œ1í•­\": text.strip()}\n",
    "\n",
    "    result = {}\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        content = text[start + 1:end].strip()\n",
    "        result[f\"ì œ{i + 1}í•­\"] = content\n",
    "    return result\n",
    "\n",
    "# íŒŒì¼ ì—´ê¸°\n",
    "with open(\"./data/cleaned_law_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "# ë²•ë¥ ëª… ë‹¨ìœ„ë¡œ ë°˜ë³µ\n",
    "for law_name, articles in raw_data.items():\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        content = article.get(\"ì¡°í•­ë‚´ìš©\", \"\")\n",
    "        article[\"ì¡°í•­ë‚´ìš©\"] = split_articles(content)\n",
    "        processed_articles.append(article)\n",
    "    processed_data[law_name] = processed_articles\n",
    "\n",
    "# ì €ì¥\n",
    "with open(\"./data/cleaned_law_data_preprocessed.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… ì¡°ë¬¸ í•­ë³„ ë¶„í•  ì™„ë£Œ! cleaned_law_data_preprocessed.json ì— ì €ì¥ë¨.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (0.2.17)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: chromadb in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (0.5.23)\n",
      "Requirement already satisfied: gradio in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (4.44.1)\n",
      "Requirement already satisfied: tiktoken in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (3.10.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain) (8.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain-openai) (1.68.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (0.115.11)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.33.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (3.21.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: graphlib_backport>=1.0.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (4.5.2)\n",
      "Requirement already satisfied: ffmpy in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (0.29.3)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (3.7.5)\n",
      "Requirement already satisfied: packaging in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydub in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (0.11.2)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: fsspec in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio-client==1.3.0->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from build>=1.0.3->chromadb) (8.5.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from fastapi>=0.95.2->chromadb) (0.44.0)\n",
      "Requirement already satisfied: certifi in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from importlib-resources->chromadb) (3.20.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
      "Requirement already satisfied: coloredlogs in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading langchain_community-0.2.19-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-openai, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.19 langchain-openai-0.1.25 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-openai chromadb gradio tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp38-cp38-macosx_10_14_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from faiss-cpu) (1.24.4)\n",
      "Requirement already satisfied: packaging in /Users/sky/Documents/skn/skn-3rd-1Team/skn3/lib/python3.8/site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp38-cp38-macosx_10_14_x86_64.whl (7.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ë²¡í„° DBê°€ ì—†ì–´ ìƒì„±í•©ë‹ˆë‹¤...\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/bf749bkd2lv_11d1rlyjcgz00000gn/T/ipykernel_34480/906641993.py:78: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = qa_chain(query)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Set API key\n",
    "\n",
    "# Load and split law data\n",
    "def load_and_split_law_data(path=\"data/cleaned_law_data_preprocessed.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    docs = []\n",
    "    for law_name, articles in data.items():\n",
    "        for article in articles:\n",
    "            ì¡°ë¬¸ë²ˆí˜¸ = article[\"ì¡°í•­ë²ˆí˜¸\"]\n",
    "            ì œëª© = article[\"ì¡°í•­ì œëª©\"]\n",
    "            ì¡°ë¬¸ë‚´ìš© = article[\"ì¡°í•­ë‚´ìš©\"]\n",
    "            for í•­, ë‚´ìš© in ì¡°ë¬¸ë‚´ìš©.items():\n",
    "                full_text = f\"[{law_name} {ì¡°ë¬¸ë²ˆí˜¸} {í•­}] {ì œëª©}\\n{ë‚´ìš©}\"\n",
    "                docs.append(Document(page_content=full_text, metadata={\n",
    "                    \"source\": f\"{law_name} {ì¡°ë¬¸ë²ˆí˜¸} {í•­}\"\n",
    "                }))\n",
    "    return docs\n",
    "\n",
    "# Load and split case data\n",
    "def load_and_split_case_data(path=\"data/filtered_case_data.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    docs = []\n",
    "    for case in data:\n",
    "        if case.get(\"íŒë¡€ë‚´ìš©\"):\n",
    "            content = json.dumps(case[\"íŒë¡€ë‚´ìš©\"], ensure_ascii=False, indent=2)\n",
    "            title = case[\"ì‚¬ê±´ëª…\"]\n",
    "            metadata = {\n",
    "                \"ì‚¬ê±´ë²ˆí˜¸\": case[\"ì‚¬ê±´ë²ˆí˜¸\"],\n",
    "                \"ì„ ê³ ì¼ì\": case[\"ì„ ê³ ì¼ì\"],\n",
    "                \"ì‚¬ê±´ëª…\": case[\"ì‚¬ê±´ëª…\"]\n",
    "            }\n",
    "            docs.append(Document(page_content=f\"[íŒë¡€] {title}\\n{content}\", metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "# Build vector DB (only once)\n",
    "def build_vectorstore():\n",
    "    law_docs = load_and_split_law_data()\n",
    "    case_docs = load_and_split_case_data()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    all_docs = splitter.split_documents(law_docs + case_docs)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectordb = FAISS.from_documents(all_docs, embedding=embeddings)\n",
    "    vectordb.save_local(\"law_case_db\")\n",
    "    return vectordb\n",
    "\n",
    "# Check if FAISS DB exists\n",
    "def initialize_vector_db():\n",
    "    if not os.path.exists(\"law_case_db/index.faiss\"):\n",
    "        print(\"ğŸ“¦ ë²¡í„° DBê°€ ì—†ì–´ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "        return build_vectorstore()\n",
    "    print(\"âœ… ë²¡í„° DB ë¡œë”© ì¤‘...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    return FAISS.load_local(\"law_case_db\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Initialize vector DB\n",
    "vectordb = initialize_vector_db()\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\"),\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Gradio UI\n",
    "def answer_query(query):\n",
    "    result = qa_chain(query)\n",
    "    sources = \"\\n\\n\".join([\n",
    "        f\"ğŸ”¹ {doc.metadata.get('source', 'ë¯¸ìƒ')}\\n{doc.page_content[:300]}...\"\n",
    "        for doc in result[\"source_documents\"]\n",
    "    ])\n",
    "    return result[\"result\"], sources\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer_query,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"ì˜ˆ: í˜¼ì¸ë¬´íš¨ í™•ì¸ì†Œì†¡ì€ ì´í˜¼ í›„ì—ë„ ì œê¸° ê°€ëŠ¥í•œê°€ìš”?\"),\n",
    "    outputs=[\"text\", \"text\"],\n",
    "    title=\"ğŸ“š ë²•ë¥ +íŒë¡€ RAG ê²€ìƒ‰ê¸°\",\n",
    "    description=\"ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ë©´ ê´€ë ¨ ë²•ì¡°ë¬¸ê³¼ íŒë¡€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì„¤ëª…í•´ì¤ë‹ˆë‹¤.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS ë²¡í„° DB ë¡œë”© ì¤‘...\n",
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# âœ… LangSmith & OpenAI í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "\n",
    "# âœ… FAISS ë²¡í„° DB ë¡œë”© í•¨ìˆ˜\n",
    "def initialize_vector_db():\n",
    "    if not os.path.exists(\"law_case_db/index.faiss\"):\n",
    "        raise FileNotFoundError(\"âŒ FAISS DB íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € build_vectorstore()ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"âœ… FAISS ë²¡í„° DB ë¡œë”© ì¤‘...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    return FAISS.load_local(\"law_case_db\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ê°€ì¡±ë²• ì „ë¬¸ AI ìƒë‹´ì‚¬ ì—­í• )\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "ë‹¹ì‹ ì€ 'ê°€ì¡±ë²• ì „ë¬¸ AI ìƒë‹´ì‚¬'ì…ë‹ˆë‹¤. ì•„ë˜ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(ë²•ë¥  ì¡°ë¬¸ ë° íŒë¡€)ì— ê·¼ê±°í•˜ì—¬ ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "â“ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ğŸ“„ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ğŸ’¬ë‹µë³€:\n",
    "- ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë²•ì  í•´ì„ ë˜ëŠ” ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "- ëª…í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "- ë²•ë¥  ìš©ì–´ëŠ” í•„ìš”í•œ ê²½ìš° ê°„ë‹¨íˆ í’€ì´í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# âœ… DB ë° ì²´ì¸ ì´ˆê¸°í™”\n",
    "vectordb = initialize_vector_db()\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\"),\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜\n",
    "def answer_query(query):\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        return \"â—ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\", \"\"\n",
    "\n",
    "    result = qa_chain(query)\n",
    "    sources = \"\\n\\n\".join([\n",
    "        f\"ğŸ”¹ {doc.metadata.get('source', 'ë¯¸ìƒ')}\\n{doc.page_content[:300]}...\"\n",
    "        for doc in result.get(\"source_documents\", [])\n",
    "    ])\n",
    "    return result.get(\"result\", \"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"), sources\n",
    "\n",
    "# âœ… Gradio UI êµ¬ì„±\n",
    "demo = gr.Interface(\n",
    "    fn=answer_query,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"ì˜ˆ: í˜‘ì˜ì´í˜¼ìœ¼ë¡œ ìœ„ìë£Œë¥¼ ë°›ì•˜ìœ¼ë©´ ì‚¬í•´í–‰ìœ„ê°€ ë  ìˆ˜ ì—†ë‚˜ìš”?\"),\n",
    "    outputs=[\"text\", \"text\"],\n",
    "    title=\"ğŸ“š ê°€ì¡±ë²• í•´ê²°ì‚¬ - ì´í˜¼ & ì–‘ìœ¡ê¶Œ ìƒë‹´ AI (LangSmith ì¶”ì  í¬í•¨)\",\n",
    "    description=\"ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ë©´ ê´€ë ¨ ë²•ì¡°ë¬¸ê³¼ íŒë¡€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì„¤ëª…í•´ì¤ë‹ˆë‹¤.\"\n",
    ")\n",
    "\n",
    "# âœ… ì•± ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì¸ íŠœë‹ ë°ì´í„° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“š Generating fine-tune data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:14<00:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# âœ… Step 0: API Key ì„¤ì •\n",
    "\n",
    "# âœ… Step 1: ë²¡í„° DB ë¡œë”©\n",
    "vectordb = FAISS.load_local(\"law_case_db\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# âœ… Step 2: RAG ê¸°ë°˜ QA ì²´ì¸ ì •ì˜\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\"),\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# âœ… Step 3: ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸ + ì²´ì¸ ì •ì˜\n",
    "question_gen_prompt = PromptTemplate.from_template(\"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë²•ë¥  ë˜ëŠ” íŒë¡€ì™€ ê´€ë ¨ëœ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\"\"\")\n",
    "\n",
    "question_gen_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7),\n",
    "    prompt=question_gen_prompt\n",
    ")\n",
    "\n",
    "# âœ… Step 4: ë¬¸ì„œ ë¡œë”© í•¨ìˆ˜\n",
    "def load_documents():\n",
    "    with open(\"data/filtered_case_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        cases = json.load(f)\n",
    "    with open(\"data/cleaned_law_data_preprocessed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        laws = json.load(f)\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # ë²•ë¥  ì¡°ë¬¸ ë¬¸ì„œ\n",
    "    for law_name, articles in laws.items():\n",
    "        for article in articles:\n",
    "            title = article[\"ì¡°í•­ì œëª©\"]\n",
    "            num = article[\"ì¡°í•­ë²ˆí˜¸\"]\n",
    "            content = article[\"ì¡°í•­ë‚´ìš©\"]\n",
    "            for í•­, í•­ë‚´ìš© in content.items():\n",
    "                text = f\"[{law_name} {num} {í•­}] {title}\\n{í•­ë‚´ìš©}\"\n",
    "                documents.append(Document(page_content=text))\n",
    "\n",
    "    # íŒë¡€ ë¬¸ì„œ\n",
    "    for case in cases:\n",
    "        if case.get(\"íŒë¡€ë‚´ìš©\"):\n",
    "            text = json.dumps(case[\"íŒë¡€ë‚´ìš©\"], ensure_ascii=False, indent=2)\n",
    "            documents.append(Document(page_content=f\"[íŒë¡€] {case['ì‚¬ê±´ëª…']}\\n{text}\"))\n",
    "\n",
    "    return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(documents)\n",
    "\n",
    "# âœ… Step 5: íŒŒì¸íŠœë‹ ë°ì´í„° ìë™ ìƒì„±\n",
    "def generate_finetune_data(docs, max_samples=100):\n",
    "    dataset = []\n",
    "    pbar = tqdm(total=max_samples, desc=\"ğŸ“š Generating fine-tune data\")\n",
    "\n",
    "    for doc in docs:\n",
    "        if len(dataset) >= max_samples:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            q_result = question_gen_chain.invoke({\"context\": doc.page_content})\n",
    "            questions_raw = q_result.get(\"text\", \"\")\n",
    "\n",
    "            questions = [\n",
    "                q.strip(\"-â€¢1234567890. )\") for q in questions_raw.split(\"\\n\")\n",
    "                if len(q.strip()) > 10\n",
    "            ][:3]\n",
    "\n",
    "            for question in questions:\n",
    "                rag_result = qa_chain.invoke(question)\n",
    "\n",
    "                # context ì¶”ì¶œ\n",
    "                source_docs = rag_result.get(\"source_documents\", [])\n",
    "                context_texts = [\n",
    "                    doc.page_content for doc in source_docs\n",
    "                    if hasattr(doc, \"page_content\")\n",
    "                ]\n",
    "                context = \"\\n\\n\".join(context_texts)\n",
    "\n",
    "                dataset.append({\n",
    "                    \"instruction\": question,\n",
    "                    \"input\": context.strip(),\n",
    "                    \"output\": rag_result[\"result\"].strip()\n",
    "                })\n",
    "\n",
    "                pbar.update(1)\n",
    "                if len(dataset) >= max_samples:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ:\", e)\n",
    "            continue\n",
    "\n",
    "    pbar.close()\n",
    "    return dataset\n",
    "\n",
    "# âœ… Step 6: ì‹¤í–‰ ë° ì €ì¥\n",
    "all_docs = load_documents()\n",
    "finetune_data = generate_finetune_data(all_docs, max_samples=100)\n",
    "\n",
    "os.makedirs(\"finetune_dataset\", exist_ok=True)\n",
    "with open(\"finetune_dataset/familylaw_finetune_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(finetune_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìœ ì˜ë¯¸í•œ ë°ì´í„°ë§Œ 165ê±´ìœ¼ë¡œ ì •ì œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ì›ë³¸ íŒŒì¼ ê²½ë¡œ\n",
    "input_path = \"finetune_dataset/familylaw_finetune_data.json\"\n",
    "output_path = \"finetune_dataset/cleaned_familylaw_finetune_data.json\"\n",
    "\n",
    "# ì œê±°í•˜ê³  ì‹¶ì€ ì¶œë ¥ í…ìŠ¤íŠ¸ íŒ¨í„´ë“¤\n",
    "remove_phrases = [\n",
    "    \"ì£„ì†¡í•˜ì§€ë§Œ\", \n",
    "    \"ì œê³µëœ ì •ë³´ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\", \n",
    "    \"ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\", \n",
    "    \"êµ¬ì²´ì ì¸ ì •ë³´ëŠ” ì•Œì§€ ëª»í•©ë‹ˆë‹¤\", \n",
    "    \"ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤\",\n",
    "    \"ì œê°€ ì•Œê³  ìˆëŠ” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ì‘ë‹µì¸ì§€ íŒë‹¨\n",
    "def is_useless_output(output: str) -> bool:\n",
    "    return any(phrase in output for phrase in remove_phrases)\n",
    "\n",
    "# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# í•„í„°ë§\n",
    "filtered_data = [\n",
    "    item for item in data\n",
    "    if not is_useless_output(item[\"output\"])\n",
    "]\n",
    "\n",
    "# ì €ì¥\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ìœ ì˜ë¯¸í•œ ë°ì´í„°ë§Œ {len(filtered_data)}ê±´ìœ¼ë¡œ ì •ì œ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì¶”ê°€ 100ê±´ ìƒì„± ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:05<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëˆ„ì  ì €ì¥ ì™„ë£Œ! ì´ 200ê±´.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ\n",
    "existing_data_path = \"finetune_dataset/familylaw_finetune_data.json\"\n",
    "with open(existing_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    existing_data = json.load(f)\n",
    "    existing_questions = set(item[\"instruction\"].strip() for item in existing_data)\n",
    "\n",
    "# âœ… ìƒˆ ë°ì´í„° ìƒì„±\n",
    "new_data = []\n",
    "pbar = tqdm(total=100, desc=\"ğŸ“Œ ì¶”ê°€ 100ê±´ ìƒì„± ì¤‘\")\n",
    "\n",
    "for doc in all_docs:\n",
    "    if len(new_data) >= 100:\n",
    "        break\n",
    "    try:\n",
    "        q_result = question_gen_chain.invoke({\"context\": doc.page_content})\n",
    "        questions_raw = q_result.get(\"text\", \"\")\n",
    "        questions = [\n",
    "            q.strip(\"-â€¢1234567890. )\") for q in questions_raw.split(\"\\n\")\n",
    "            if len(q.strip()) > 10\n",
    "        ][:3]\n",
    "\n",
    "        for question in questions:\n",
    "            if question.strip() in existing_questions:\n",
    "                continue\n",
    "\n",
    "            rag_result = qa_chain.invoke(question)\n",
    "            source_docs = rag_result.get(\"source_documents\", [])\n",
    "            context_texts = [\n",
    "                doc.page_content for doc in source_docs\n",
    "                if hasattr(doc, \"page_content\")\n",
    "            ]\n",
    "            context = \"\\n\\n\".join(context_texts)\n",
    "\n",
    "            new_data.append({\n",
    "                \"instruction\": question,\n",
    "                \"input\": context.strip(),\n",
    "                \"output\": rag_result[\"result\"].strip()\n",
    "            })\n",
    "\n",
    "            existing_questions.add(question.strip())\n",
    "            pbar.update(1)\n",
    "            if len(new_data) >= 100:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ:\", e)\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# âœ… ê¸°ì¡´ ë°ì´í„° + ìƒˆ ë°ì´í„° ì €ì¥\n",
    "combined_data = existing_data + new_data\n",
    "with open(existing_data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ëˆ„ì  ì €ì¥ ì™„ë£Œ! ì´ {len(combined_data)}ê±´.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì¸íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# âœ… 1. ë°ì´í„° ë¡œë“œ\n",
    "with open(\"cleaned_familylaw_finetune_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# âœ… 2. Dataset ë³€í™˜\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# âœ… 3. Tokenizer ë¡œë“œ (ì˜ˆ: Mistral or LLaMA3 or other open model)\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # í•„ìš”í•œ ê²½ìš°\n",
    "\n",
    "# âœ… 4. Prompt í¬ë§· (instruction + input + output ì—°ê²°)\n",
    "def format_example(example):\n",
    "    prompt = f\"### ì§ˆë¬¸:\\n{example['instruction']}\\n\\n### ë¬¸ì„œ:\\n{example['input']}\\n\\n### ë‹µë³€:\"\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=1024)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(example[\"output\"], truncation=True, padding=\"max_length\", max_length=256)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_example)\n",
    "\n",
    "# âœ… 5. ëª¨ë¸ ë¡œë“œ ë° QLoRA ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# âœ… 6. LoRA ì„¤ì •\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# âœ… 7. í•™ìŠµ ì¸ì\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_familylaw\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# âœ… 8. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# âœ… 9. íŒŒì¸íŠœë‹ ì‹œì‘\n",
    "trainer.train()\n",
    "\n",
    "# âœ… 10. ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./qlora_familylaw/peft_model\")\n",
    "tokenizer.save_pretrained(\"./qlora_familylaw/peft_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# âœ… Hugging Face ë¡œê·¸ì¸ (í† í° ì…ë ¥)\n",
    "login(\"\")\n",
    "\n",
    "# âœ… CUDA í™˜ê²½ ì„¤ì • (GPU 1ê°œë§Œ ì‚¬ìš©)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë“œ\n",
    "with open(\"./data/cleaned_familylaw_finetune_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# âœ… ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_id = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def format_prompt(example):\n",
    "    prompt = f\"### ì§ˆë¬¸:\\n{example['instruction']}\\n\\n### ë¬¸ì„œ:\\n{example['input']}\\n\\n### ë‹µë³€:\"\n",
    "    response = example[\"output\"]\n",
    "    full_text = prompt + \" \" + response\n",
    "\n",
    "    # ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ë¡œ ì²˜ë¦¬\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # ë ˆì´ë¸”ì€ input_ids ë³µì‚¬í•´ì„œ ë‹µë³€ ì „ê¹Œì§€ëŠ” ë§ˆìŠ¤í‚¹\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # prompt ê¸¸ì´ë§Œí¼ -100 ë§ˆìŠ¤í‚¹\n",
    "    prompt_len = len(tokenizer(prompt, truncation=True, max_length=1024)[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt)\n",
    "\n",
    "# âœ… QLoRA ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë“œ (GPU 0ë§Œ ì‚¬ìš©)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# âœ… LoRA ì„¤ì •\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# âœ… í•™ìŠµ ì¸ì\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_openchat_familylaw\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# âœ… Trainer ì •ì˜ ë° í•™ìŠµ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# âœ… ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./qlora_openchat_familylaw/peft_model\")\n",
    "tokenizer.save_pretrained(\"./qlora_openchat_familylaw/peft_model\")\n",
    "\n",
    "# âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸ìš© ì¶”ë¡  í•¨ìˆ˜\n",
    "def infer(instruction, context=\"\"):\n",
    "    model.eval()\n",
    "    prompt = f\"### ì§ˆë¬¸:\\n{instruction}\\n\\n### ë¬¸ì„œ:\\n{context}\\n\\n### ë‹µë³€:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# âœ… ì˜ˆì‹œ ì¶”ë¡ \n",
    "print(infer(\"ì¹œê¶Œê³¼ ì–‘ìœ¡ê¶Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
