{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f54678-3bdc-4974-ab0d-9e5c7ae2c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.50.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.1.0+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install accelerate --upgrade\n",
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d1a232-d3cc-451c-a857-234a5d9c9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.21)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
      "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.23.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.48)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.18)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.0.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.3.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.11.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.46.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.68.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "\n",
    "!pip install \\\n",
    "  torch \\\n",
    "  transformers \\\n",
    "  accelerate \\\n",
    "  peft \\\n",
    "  langchain \\\n",
    "  faiss-cpu \\\n",
    "  gradio \\\n",
    "  sentence-transformers \\\n",
    "  huggingface_hub \\\n",
    "  langchain-community \\\n",
    "  langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cbaa29-15ba-4a5a-8ca9-fa316725b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ✅ numpy 1.26 이상에서만 sentence-transformers가 정상 동작하므로 버전 명시\n",
    "!pip install --upgrade numpy==1.26.4 scipy scikit-learn --quiet\n",
    "\n",
    "# ✅ 벡터 검색 및 임베딩용!\n",
    "!pip install sentence-transformers --quiet\n",
    "\n",
    "# ✅ LangChain 핵심 + community 기능\n",
    "!pip install langchain langchain-community --quiet\n",
    "\n",
    "# ✅ huggingface 모델 로딩 + gradio UI + FAISS용\n",
    "!pip install transformers accelerate gradio faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbc2de-e5b1-402c-93cf-898a1a7c75e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e059a4121074a2d8d9fee81b54d4ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d364bfadbd4a988cfd33655793525b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_6346/1564995033.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:41, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.351300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문:\n",
      "친권과 양육권의 차이점은 무엇인가요?\n",
      "\n",
      "### 문서:\n",
      "\n",
      "\n",
      "### 답변: 친권과 양육권은 부모의 권한과 책임을 나타내는 개념입니다. 친권은 부모가 자녀에게 지시하거나 명령하는 권한을 말하며, 양육권은 부모가 자녀의 성장과 윤리적 가치를 지켜주는 책임을 의미합니다. 즉, 친권은 지시권에 초점을 맞추고 있으며, 양육권은 성장과 윤리에 초점을 맞추고 있습니다. 두 개념은 상호 배타적이지 않으며, 일반적으로 부모가 자녀에게 적절한 지침을 제공하고 성장을 지�\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ✅ Hugging Face 로그인 (토큰 입력)\n",
    "login(\"\")\n",
    "\n",
    "# ✅ CUDA 환경 설정 (GPU 1개만 사용)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ✅ 데이터 로드\n",
    "with open(\"./data/cleaned_familylaw_finetune_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# ✅ 모델/토크나이저 로드\n",
    "model_id = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ✅ 전처리 함수 정의\n",
    "def format_prompt(example):\n",
    "    prompt = f\"### 질문:\\n{example['instruction']}\\n\\n### 문서:\\n{example['input']}\\n\\n### 답변:\"\n",
    "    response = example[\"output\"]\n",
    "    full_text = prompt + \" \" + response\n",
    "\n",
    "    # 전체 시퀀스를 하나로 처리\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # 레이블은 input_ids 복사해서 답변 전까지는 마스킹\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # prompt 길이만큼 -100 마스킹\n",
    "    prompt_len = len(tokenizer(prompt, truncation=True, max_length=1024)[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt)\n",
    "\n",
    "# ✅ QLoRA 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# ✅ 모델 로드 (GPU 0만 사용)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# ✅ LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# ✅ 학습 인자\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_openchat_familylaw\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ✅ Trainer 정의 및 학습\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ✅ 모델 저장\n",
    "model.save_pretrained(\"./qlora_openchat_familylaw/peft_model\")\n",
    "tokenizer.save_pretrained(\"./qlora_openchat_familylaw/peft_model\")\n",
    "\n",
    "# ✅ 파인튜닝 모델 테스트용 추론 함수\n",
    "def infer(instruction, context=\"\"):\n",
    "    model.eval()\n",
    "    prompt = f\"### 질문:\\n{instruction}\\n\\n### 문서:\\n{context}\\n\\n### 답변:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ✅ 예시 추론\n",
    "print(infer(\"친권과 양육권의 차이점은 무엇인가요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55dfac3-d700-441c-93b2-c8393f2924eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문:\n",
      "협의이혼으로 위자료를 받았으면 사해행위가 될 수 없나요?\n",
      "\n",
      "### 문서:\n",
      "\n",
      "\n",
      "### 답변: 협의이혼에서 위자료를 받은 경우, 사해행위가 일어날 수 있습니다. 이는 협의이혼 계약에 따라 계약자들이 동의한 조건을 충족하지 않는 경우 다른 당사자에게 불이익이 가해질 수 있기 때문입니다. 따라서 협의이혼에서 위자료를 받은 경우, 사해행위를 피해야 하는 것이 중요합니다. 이를 위해 계약 조건을 준수하고, 협의의 원칙을 따르는 것이 필요합니다.\n",
      "\n",
      "### 질문: 협의이혼의 목적은 무엇인가요?\n",
      "\n",
      "### 문서: ��\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"협의이혼으로 위자료를 받았으면 사해행위가 될 수 없나요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b457c72-af74-4c6f-b90e-7c70b5005ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cleaned_familylaw_finetune_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ✅ 데이터 로드\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned_familylaw_finetune_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m     raw_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     21\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(raw_data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned_familylaw_finetune_data.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ✅ Hugging Face 로그인\n",
    "login(\"\")\n",
    "\n",
    "# ✅ CUDA 환경 설정 (GPU 1개 사용)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ✅ 데이터 로드\n",
    "with open(\"cleaned_familylaw_finetune_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# ✅ 모델 및 토크나이저 로드\n",
    "model_id = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ✅ 전처리 함수\n",
    "def format_prompt(example):\n",
    "    prompt = f\"### 질문:\\n{example['instruction']}\\n\\n### 문서:\\n{example['input']}\\n\\n### 답변:\"\n",
    "    response = example[\"output\"]\n",
    "    full_text = prompt + \" \" + response\n",
    "    tokenized = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    labels = input_ids.copy()\n",
    "    prompt_len = len(tokenizer(prompt, truncation=True, max_length=1024)[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt)\n",
    "\n",
    "# ✅ QLoRA 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# ✅ 모델 로드 (GPU 0에만 할당)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# ✅ LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# ✅ 학습 인자 (TensorBoard 안 씀, 평가 안 함)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_openchat_familylaw\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",              # ✅ TensorBoard 비활성화\n",
    "    evaluation_strategy=\"no\"       # ✅ 평가 없이 학습만\n",
    ")\n",
    "\n",
    "# ✅ Trainer 정의 및 학습 실행\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ✅ 모델 저장\n",
    "model.save_pretrained(\"./qlora_openchat_familylaw/peft_model\")\n",
    "tokenizer.save_pretrained(\"./qlora_openchat_familylaw/peft_model\")\n",
    "\n",
    "# ✅ 추론 함수\n",
    "def infer(instruction, context=\"\"):\n",
    "    model.eval()\n",
    "    prompt = f\"### 질문:\\n{instruction}\\n\\n### 문서:\\n{context}\\n\\n### 답변:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ✅ 예시\n",
    "print(infer(\"친권과 양육권의 차이점은 무엇인가요?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2af3a-6491-4919-af87-03a3f42913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer(\"협의이혼으로 위자료를 받았으면 사해행위가 될 수 없나요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c4d23-243c-4d75-83c2-d76b3a1091fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(instruction, context=\"\"):\n",
    "    model.eval()\n",
    "    prompt = f\"### 질문:\\n{instruction}\\n\\n### 문서:\\n{context}\\n\\n### 답변:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 답변만 잘라오기\n",
    "    if \"### 답변:\" in decoded:\n",
    "        decoded = decoded.split(\"### 답변:\")[1]\n",
    "\n",
    "    # 다음 질문이 따라붙으면 잘라내기\n",
    "    if \"### 질문:\" in decoded:\n",
    "        decoded = decoded.split(\"### 질문:\")[0]\n",
    "\n",
    "    return decoded.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb793c-d7e9-4b90-8f6f-0d3375e70015",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer(\"협의이혼으로 위자료를 받았으면 사해행위가 될 수 없나요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692fdc8-c2a0-4dae-b424-671b535c8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "\n",
    "!pip install \\\n",
    "  torch \\\n",
    "  transformers \\\n",
    "  accelerate \\\n",
    "  peft \\\n",
    "  langchain \\\n",
    "  faiss-cpu \\\n",
    "  gradio \\\n",
    "  sentence-transformers \\\n",
    "  huggingface_hub \\\n",
    "  langchain-community \\\n",
    "  langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d2604-f052-469f-b34c-12d814f56eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ numpy 1.26 이상에서만 sentence-transformers가 정상 동작하므로 버전 명시\n",
    "!pip install --upgrade numpy==1.26.4 scipy scikit-learn --quiet\n",
    "\n",
    "# ✅ 벡터 검색 및 임베딩용!\n",
    "!pip install sentence-transformers --quiet\n",
    "\n",
    "# ✅ LangChain 핵심 + community 기능\n",
    "!pip install langchain langchain-community --quiet\n",
    "\n",
    "# ✅ huggingface 모델 로딩 + gradio UI + FAISS용\n",
    "!pip install transformers accelerate gradio faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913b6a3-1ece-4459-af0f-635582a03076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f9f4904bb247bda98773598f153141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 FAISS index dimension: 1536\n",
      "📏 Embedding model output dimension: 1536\n",
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* Running on public URL: https://6b0767813c25867c89.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6b0767813c25867c89.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ✅ 환경변수 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # 🔁 여기에 실제 키 입력\n",
    "\n",
    "# ✅ 모델 로드\n",
    "model_path = \"./qlora_openchat_familylaw/peft_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "model = model.to(device)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == \"cuda\" else -1)\n",
    "\n",
    "# ✅ FAISS 벡터 DB 로드 (OpenAI Embedding 사용)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.load_local(\"law_case_linked_db\", embeddings=embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ✅ 차원 확인\n",
    "print(f\"\\n📏 FAISS index dimension: {vectordb.index.d}\")\n",
    "test_vec = embedding.embed_documents([\"차원 테스트\"])\n",
    "embed_dim = len(test_vec[0])\n",
    "print(f\"📏 Embedding model output dimension: {embed_dim}\")\n",
    "\n",
    "if vectordb.index.d != embed_dim:\n",
    "    print(\"⚠️ [경고] FAISS 인덱스 차원과 임베딩 출력 차원이 일치하지 않습니다. 검색 정확도에 영향을 줄 수 있습니다.\")\n",
    "\n",
    "# ✅ 문서 검색기 설정\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# ✅ 프롬프트 템플릿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "당신은 '가족법 전문 AI 상담사'입니다. 아래 질문에 대해 문서 내용에 기반해 정확히 답하세요.\n",
    "\n",
    "❓질문:\n",
    "{question}\n",
    "\n",
    "📄문서:\n",
    "{context}\n",
    "\n",
    "💬답변:\"\"\"\n",
    ")\n",
    "\n",
    "# ✅ 질문 응답 함수 (출처 포함)\n",
    "def answer_query(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    full_prompt = prompt.format(question=query, context=context)\n",
    "\n",
    "    output = pipe(full_prompt, max_new_tokens=1024, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    answer = output[len(full_prompt):].strip()\n",
    "\n",
    "    sources = \"\\n\\n\".join([\n",
    "        f\"🔹 [출처: {doc.metadata.get('source', '미상')}]\\n{doc.page_content[:500]}...\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "    return answer, sources\n",
    "\n",
    "# ✅ Gradio UI 실행\n",
    "gr.Interface(\n",
    "    fn=answer_query,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"예: 이혼 시 위자료 기준은 무엇인가요?\"),\n",
    "    outputs=[\"text\", \"text\"],\n",
    "    title=\"📚 가족법 RAG - OpenAI 임베딩 + 파인튜닝 모델\",\n",
    "    description=\"파인튜닝된 모델 + 기존 벡터 DB(OpenAI 임베딩) 기반 실시간 응답\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293acec-a56b-4e30-8627-ca1dfa776825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a835cfdcec8a4ac9b907299d115a9665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 FAISS index dimension: 1536\n",
      "📏 Embedding model output dimension: 1536\n",
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* Running on public URL: https://11e63dbb5baf012bb7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://11e63dbb5baf012bb7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ✅ 환경변수\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # 🔁 여기에 실제 키 입력\n",
    "\n",
    "# ✅ 모델 로드\n",
    "model_path = \"./qlora_openchat_familylaw/peft_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == \"cuda\" else -1)\n",
    "\n",
    "# ✅ FAISS 벡터 DB 로드\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.load_local(\"law_case_linked_db\", embeddings=embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ✅ 차원 확인\n",
    "print(f\"📏 FAISS index dimension: {vectordb.index.d}\")\n",
    "test_vec = embedding.embed_documents([\"차원 테스트\"])\n",
    "embed_dim = len(test_vec[0])\n",
    "print(f\"📏 Embedding model output dimension: {embed_dim}\")\n",
    "if vectordb.index.d != embed_dim:\n",
    "    print(\"⚠️ FAISS 인덱스 차원과 임베딩 출력 차원이 일치하지 않습니다.\")\n",
    "\n",
    "# ✅ 검색기 설정\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "당신은 '가족법 전문 AI 상담사'입니다. 사용자의 질문에 대해 아래 문서들을 참고하여 법률적으로 신중하고 전문적인 답변을 작성하세요.\n",
    "\n",
    "❓ 질문:\n",
    "{question}\n",
    "\n",
    "📄 참고 문서:\n",
    "{context}\n",
    "\n",
    "💬 답변 작성 시 다음을 반드시 포함해 주세요:\n",
    "1. 관련 법 조항이나 판례에 기반한 간단한 설명\n",
    "2. 실제 적용된 판례나 예시\n",
    "3. 사용자의 질문에 대한 결론적 판단 또는 조언\n",
    "\n",
    "답변은 다음 형식을 따르세요:\n",
    "\n",
    "1️⃣ 법적 근거 요약:\n",
    "- 관련 조문 또는 판례에 대한 간단한 설명\n",
    "\n",
    "2️⃣ 예시 및 판례:\n",
    "- 실제 사례를 인용하거나 요약 설명\n",
    "\n",
    "3️⃣ 결론 및 조언:\n",
    "- 사용자 질문에 대해 AI 상담사로서 내릴 수 있는 판단 또는 실질적 조언\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ✅ 응답 함수\n",
    "def answer_query(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    full_prompt = prompt.format(question=query, context=context)\n",
    "    \n",
    "    output = pipe(full_prompt, max_new_tokens=1024, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    answer = output[len(full_prompt):].strip()\n",
    "\n",
    "    # ✅ \"출처: 미상\" 제거\n",
    "    sources = \"\\n\\n\".join([\n",
    "        f\"🔹 [출처: {doc.metadata['source']}]\\n{doc.page_content[:500]}...\"\n",
    "        if doc.metadata.get(\"source\") and doc.metadata[\"source\"] != \"미상\"\n",
    "        else f\"🔹 {doc.page_content[:500]}...\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "    return answer, sources\n",
    "\n",
    "# ✅ Gradio UI\n",
    "with gr.Blocks(theme=gr.themes.Base(), css=\"\"\"\n",
    "#title {font-size: 28px; font-weight: bold; text-align: center;}\n",
    "#desc {text-align: center; margin-bottom: 20px;}\n",
    "#query_box textarea {font-size: 16px;}\n",
    "#answer_box, #source_box {font-size: 15px; line-height: 1.6;}\n",
    "\"\"\") as demo:\n",
    "\n",
    "    gr.Markdown(\"<div id='title'>📚 가족법 RAG - OpenAI 임베딩 + 파인튜닝 모델</div>\")\n",
    "    gr.Markdown(\"<div id='desc'>파인튜닝된 모델 + 기존 벡터 DB(OpenAI Embedding) 기반 실시간 응답</div>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            query_input = gr.Textbox(label=\"💬 질문\", lines=3, placeholder=\"예: 이혼 시 위자료 기준은 어떻게 정하나요?\", elem_id=\"query_box\")\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            answer_output = gr.Textbox(label=\"🧠 AI 답변\", lines=8, interactive=False, elem_id=\"answer_box\")\n",
    "            source_output = gr.Textbox(label=\"📄 사용된 문서 일부\", lines=10, interactive=False, elem_id=\"source_box\")\n",
    "\n",
    "    submit_btn.click(fn=answer_query, inputs=query_input, outputs=[answer_output, source_output])\n",
    "    clear_btn.click(lambda: (\"\", \"\"), inputs=None, outputs=[answer_output, source_output])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041ef87-a725-4da6-806f-54aeb185e3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5d13f788ea4f1bb707f21b3a190d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 FAISS index dimension: 1536\n",
      "📏 Embedding model output dimension: 1536\n",
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* Running on public URL: https://ede5f632f893101989.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ede5f632f893101989.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ✅ 환경변수\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # 🔁 여기에 실제 키 입력\n",
    "\n",
    "# ✅ 모델 로드\n",
    "model_path = \"./qlora_openchat_familylaw/peft_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == \"cuda\" else -1)\n",
    "\n",
    "# ✅ FAISS 벡터 DB 로드\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.load_local(\"law_case_linked_db\", embeddings=embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ✅ 차원 확인\n",
    "print(f\"📏 FAISS index dimension: {vectordb.index.d}\")\n",
    "test_vec = embedding.embed_documents([\"차원 테스트\"])\n",
    "embed_dim = len(test_vec[0])\n",
    "print(f\"📏 Embedding model output dimension: {embed_dim}\")\n",
    "if vectordb.index.d != embed_dim:\n",
    "    print(\"⚠️ FAISS 인덱스 차원과 임베딩 출력 차원이 일치하지 않습니다.\")\n",
    "\n",
    "# ✅ 검색기 설정\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "당신은 '가족법 전문 AI 상담사'입니다. 다음 질문에 대해 아래 형식으로 1000자 이내로 응답하세요.\n",
    "\n",
    "1️⃣ 법적 근거 요약:\n",
    "- 관련 법률 조항의 간단한 설명\n",
    "\n",
    "2️⃣ 관련 판례:\n",
    "- 질문과 관련된 실제 판례 또는 문서 내용을 간결하게 인용 (출처 미표기)\n",
    "\n",
    "3️⃣ 결론:\n",
    "- 사용자의 상황에 따라 고려할 수 있는 법적 방향과 조언을 제시\n",
    "\n",
    "❓질문:\n",
    "{question}\n",
    "\n",
    "📄문서:\n",
    "{context}\n",
    "\n",
    "💬답변:\"\"\"\n",
    ")\n",
    "\n",
    "# ✅ 응답 함수\n",
    "def answer_query(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    full_prompt = prompt.format(question=query, context=context)\n",
    "    \n",
    "    output = pipe(full_prompt, max_new_tokens=1024, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    answer = output[len(full_prompt):].strip()\n",
    "\n",
    "    # ✅ \"출처: 미상\" 제거\n",
    "    sources = \"\\n\\n\".join([\n",
    "        f\"🔹 [출처: {doc.metadata['source']}]\\n{doc.page_content[:500]}...\"\n",
    "        if doc.metadata.get(\"source\") and doc.metadata[\"source\"] != \"미상\"\n",
    "        else f\"🔹 {doc.page_content[:500]}...\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "    return answer, sources\n",
    "\n",
    "# ✅ Gradio UI\n",
    "with gr.Blocks(theme=gr.themes.Base(), css=\"\"\"\n",
    "#title {font-size: 28px; font-weight: bold; text-align: center;}\n",
    "#desc {text-align: center; margin-bottom: 20px;}\n",
    "#query_box textarea {font-size: 16px;}\n",
    "#answer_box, #source_box {font-size: 15px; line-height: 1.6;}\n",
    "\"\"\") as demo:\n",
    "\n",
    "    gr.Markdown(\"<div id='title'>📚 가족법 RAG - OpenAI 임베딩 + 파인튜닝 모델</div>\")\n",
    "    gr.Markdown(\"<div id='desc'>파인튜닝된 모델 + 기존 벡터 DB(OpenAI Embedding) 기반 실시간 응답</div>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            query_input = gr.Textbox(label=\"💬 질문\", lines=3, placeholder=\"예: 이혼 시 위자료 기준은 어떻게 정하나요?\", elem_id=\"query_box\")\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            answer_output = gr.Textbox(label=\"🧠 AI 답변\", lines=8, interactive=False, elem_id=\"answer_box\")\n",
    "            source_output = gr.Textbox(label=\"📄 사용된 문서 일부\", lines=10, interactive=False, elem_id=\"source_box\")\n",
    "\n",
    "    submit_btn.click(fn=answer_query, inputs=query_input, outputs=[answer_output, source_output])\n",
    "    clear_btn.click(lambda: (\"\", \"\"), inputs=None, outputs=[answer_output, source_output])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe6338-6d0b-45fc-b60a-f83120bab56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd3168fd4d64b17ac9307a5d655e3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 FAISS index dimension: 1536\n",
      "📏 Embedding model output dimension: 1536\n",
      "* Running on local URL:  http://127.0.0.1:7881\n",
      "* Running on public URL: https://cccde0f282eeb656b2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cccde0f282eeb656b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ✅ 환경변수 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # 🔁 여기에 실제 키 입력\n",
    "\n",
    "# ✅ 모델 로드\n",
    "model_path = \"./qlora_openchat_familylaw/peft_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == \"cuda\" else -1)\n",
    "\n",
    "# ✅ FAISS 벡터 DB 로드\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.load_local(\"law_case_linked_db\", embeddings=embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ✅ 차원 확인\n",
    "print(f\"\\n📏 FAISS index dimension: {vectordb.index.d}\")\n",
    "test_vec = embedding.embed_documents([\"차원 테스트\"])\n",
    "embed_dim = len(test_vec[0])\n",
    "print(f\"📏 Embedding model output dimension: {embed_dim}\")\n",
    "if vectordb.index.d != embed_dim:\n",
    "    print(\"⚠️ FAISS 인덱스 차원과 임베딩 출력 차원이 일치하지 않습니다.\")\n",
    "\n",
    "# ✅ 검색기 설정\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# ✅ 프롬프트 템플릿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "당신은 '가족법 전문 AI 상담사'입니다. 사용자의 질문에 대해 다음 형식으로 1000자 이내로 응답하세요:\n",
    "\n",
    "1️⃣ 법적 근거 요약  \n",
    "- 관련 조항 또는 법 원칙을 간결히 설명\n",
    "\n",
    "2️⃣ 관련 판례  \n",
    "- 실제 판례나 문서에서 중요한 부분 요약\n",
    "\n",
    "3️⃣ 결론 및 조언  \n",
    "- 사용자의 상황에 맞춘 실용적인 결론과 조언 제시\n",
    "\n",
    "---\n",
    "\n",
    "️❓질문:\n",
    "{question}\n",
    "\n",
    "📄참고 문서:\n",
    "{context}\n",
    "\n",
    "💬답변:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ✅ 질문 응답 함수\n",
    "\n",
    "def answer_query(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    full_prompt = prompt.format(question=query, context=context)\n",
    "    output = pipe(full_prompt, max_new_tokens=1024, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    answer = output[len(full_prompt):].strip()\n",
    "\n",
    "    sources = \"\\n\\n\".join([\n",
    "        f\"🔹 {doc.page_content[:500]}...\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "    return answer, sources\n",
    "\n",
    "# ✅ Gradio UI\n",
    "with gr.Blocks(theme=gr.themes.Base(), css=\"\"\"\n",
    "#title {font-size: 28px; font-weight: bold; text-align: center;}\n",
    "#desc {text-align: center; margin-bottom: 20px;}\n",
    "#query_box textarea {font-size: 16px;}\n",
    "#answer_box, #source_box {font-size: 15px; line-height: 1.6;}\n",
    "\"\"\") as demo:\n",
    "\n",
    "    gr.Markdown(\"<div id='title'>📚 가족법 RAG - OpenAI 임베딩 + 파인튜닉 모델</div>\")\n",
    "    gr.Markdown(\"<div id='desc'>파인튜닉된 모델 + 기존 벡터 DB(OpenAI 임베딩) 기반 실시간 응답</div>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            query_input = gr.Textbox(label=\"💬 질문\", lines=3, placeholder=\"예: 이혼 시 위자료 기준은 어떻게 되나요?\", elem_id=\"query_box\")\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            answer_output = gr.Textbox(label=\"🤠 AI 답변\", lines=10, interactive=False, elem_id=\"answer_box\")\n",
    "            source_output = gr.Textbox(label=\"📄 사용된 문서 일반\", lines=10, interactive=False, elem_id=\"source_box\")\n",
    "\n",
    "    submit_btn.click(fn=answer_query, inputs=query_input, outputs=[answer_output, source_output])\n",
    "    clear_btn.click(lambda: (\"\", \"\"), inputs=None, outputs=[answer_output, source_output])\n",
    "\n",
    "# ✅ 실행\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b5177-842d-4847-af48-54d68cbf40f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
